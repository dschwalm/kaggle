{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns  \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn import tree  \n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from sklearn.cross_validation import cross_val_score  \n",
    "from sklearn import metrics \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "feature_cols = [\"teacher_prefix\",\"school_state\",\"project_grade_category\",\"project_subject_categories\",\\\n",
    "                \"teacher_number_of_previously_posted_projects\",\"project_title\",\"project_essay_1\",\\\n",
    "                \"project_essay_2\",\"project_essay_3\",\"project_essay_4\",\"project_resource_summary\",\\\n",
    "                'project_submitted_datetime']\n",
    "target_cols = [\"project_is_approved\"]\n",
    "train=pd.read_csv(\"/Users/schwalmdaniel/hullaballoo_android/ml/donors_train.csv\"\\\n",
    "                  ,usecols=['id'] + feature_cols + target_cols, converters={\"project_essay_1\":str,\\\n",
    "                    \"project_essay_2\":str,\"project_essay_3\":str,\"project_essay_4\":str,\"project_resource_summary\":str})\n",
    "test=pd.read_csv(\"/Users/schwalmdaniel/hullaballoo_android/ml/donors_test.csv\",usecols=['id'] + feature_cols,\\\n",
    "                    converters={\"project_essay_1\":str,\\\n",
    "                    \"project_essay_2\":str,\"project_essay_3\":str,\"project_essay_4\":str,\"project_resource_summary\":str})\n",
    "resources=pd.read_csv(\"/Users/schwalmdaniel/hullaballoo_android/ml/resources.csv\",usecols=['id','price',\\\n",
    "                                                                                           'quantity','description'])\n",
    "\n",
    "resources['description'].fillna('unknown',inplace=True)\n",
    "\n",
    "\n",
    "resources_price = pd.DataFrame(resources[['id', 'price']].groupby('id').price.agg(\\\n",
    "    [\n",
    "        'count', \n",
    "        'sum', \n",
    "        'min', \n",
    "        'max', \n",
    "        'mean', \n",
    "        'std', \n",
    "        #'median',\n",
    "        lambda x: len(np.unique(x)),\n",
    "    ])).reset_index()\n",
    "#print(resources_price.head())\n",
    "resources_quantity = pd.DataFrame(resources[['id', 'quantity']].groupby('id').quantity.agg(\\\n",
    "    [ \n",
    "        'sum', \n",
    "        'min', \n",
    "        'max', \n",
    "        'mean', \n",
    "        'std', \n",
    "        #'median',\n",
    "        lambda x: len(np.unique(x)),\n",
    "    ])).reset_index()\n",
    "#print(resources_quantity.head())\n",
    "\n",
    "resources_price['mean_price'] = resources_price['sum']/resources_price['count']\n",
    "resources_price['price_max_to_price_min'] = resources_price['max']/resources_price['min']\n",
    "resources_quantity['quantity_max_to_quantity_min'] = resources_quantity['max']/resources_quantity['min']\n",
    "\n",
    "resources_desc = resources.groupby(['id'])['description'].apply(' '.join).reset_index()\n",
    "                              \n",
    "seasons = [0,0,1,1,1,2,2,2,3,3,3,0] #dec - feb is winter, then spring, summer, fall etc\n",
    "\n",
    "test=test.merge(resources_price,on='id', how='left')\n",
    "test=test.merge(resources_quantity,on='id', how='left')\n",
    "test=test.merge(resources_desc,on='id', how='left')\n",
    "test['project_title_len'] = test['project_title'].apply(lambda x: len(x))\n",
    "test['project_title_wc'] = test['project_title'].apply(lambda x: len(x.split()))\n",
    "#test['project_title_uwc'] = test['project_title'].apply(lambda x: len(set(x.lower().split())))\n",
    "#test['project_title_wc_uwc_ratio'] = test['project_title_uwc'] / test['project_title_wc']\n",
    "test['project_essay_1_len'] = test['project_essay_1'].apply(lambda x: len(x))\n",
    "test['project_essay_1_wc'] = test['project_essay_1'].apply(lambda x: len(x.split()))\n",
    "#test['project_essay_1_uwc'] = test['project_essay_1'].apply(lambda x: len(set(x.lower().split())))\n",
    "#test['project_essay_1_uwc_ratio'] = test['project_essay_1_uwc'] / test['project_essay_1_wc']\n",
    "test['project_essay_2_len'] = test['project_essay_2'].apply(lambda x: len(x))\n",
    "test['project_essay_2_wc'] = test['project_essay_2'].apply(lambda x: len(x.split()))\n",
    "#test['project_essay_2_uwc'] = test['project_essay_2'].apply(lambda x: len(set(x.lower().split())))\n",
    "#test['project_essay_2_uwc_ratio'] = test['project_essay_2_uwc'] / test['project_essay_2_wc']\n",
    "test['project_essay_3_len'] = test['project_essay_3'].apply(lambda x: len(x))\n",
    "test['project_essay_3_wc'] = test['project_essay_3'].apply(lambda x: len(x.split()))\n",
    "#test['project_essay_3_uwc'] = test['project_essay_3'].apply(lambda x: len(set(x.lower().split())))\n",
    "#test['project_essay_3_uwc_ratio'] = test['project_essay_3_uwc'] / test['project_essay_3_wc']\n",
    "test['project_essay_4_len'] = test['project_essay_4'].apply(lambda x: len(x))\n",
    "test['project_essay_4_wc'] = test['project_essay_4'].apply(lambda x: len(x.split()))\n",
    "#test['project_essay_4_uwc'] = test['project_essay_4'].apply(lambda x: len(set(x.lower().split())))\n",
    "#test['project_essay_4_uwc_ratio'] = test['project_essay_4_uwc'] / test['project_essay_4_wc']\n",
    "test['project_resource_summary_len'] = test['project_resource_summary'].apply(lambda x: len(x))\n",
    "test['project_resource_summary_wc'] = test['project_resource_summary'].apply(lambda x: len(x.split()))\n",
    "#test['project_resource_summary_uwc'] = test['project_resource_summary'].apply(lambda x: len(set(x.lower().split())))\n",
    "#test['project_resource_summary_uwc_ratio'] = test['project_resource_summary_uwc'] / test['project_resource_summary_wc']\n",
    "test['description_len'] = test['description'].apply(lambda x: len(x))\n",
    "test['description_wc'] = test['description'].apply(lambda x: len(x.split()))\n",
    "test['previous_projects_0'] = test['teacher_number_of_previously_posted_projects'].apply(lambda x: 1 if x == 0 else 0)\n",
    "test['previous_projects_1_5'] = test['teacher_number_of_previously_posted_projects'].apply(lambda x: 1 if 6 > x > 0 else 0)\n",
    "test['previous_projects_6_25'] = test['teacher_number_of_previously_posted_projects'].apply(lambda x: 1 if 26 > x > 5 else 0)\n",
    "test['previous_projects_26_50'] = test['teacher_number_of_previously_posted_projects'].apply(lambda x: 1 if 51 > x > 25 else 0)\n",
    "test['previous_projects_50'] = test['teacher_number_of_previously_posted_projects'].apply(lambda x: 1 if  x > 50 else 0)\n",
    "test['project_submitted_year'] = test['project_submitted_datetime'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).year)\n",
    "test['project_submitted_quarter'] = test['project_submitted_datetime'].apply(lambda x: (datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).month-1)//3 + 1)\n",
    "test['project_submitted_yearmonth'] = test['project_submitted_datetime'].apply(lambda x: int(str(datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).year) + str(datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).month)) )\n",
    "test['project_submitted_month'] = test['project_submitted_datetime'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).month)\n",
    "test['project_submitted_weekday'] = test['project_submitted_datetime'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).weekday())\n",
    "test['project_submitted_hour'] = test['project_submitted_datetime'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).hour)\n",
    "test['project_submitted_season'] = test['project_submitted_datetime'].apply(lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).month-1)])\n",
    "test[\"gender\"] = None\n",
    "test.loc[test['teacher_prefix'] == \"Mr.\",\"gender\"] = \"Male\"\n",
    "test.loc[test['teacher_prefix'] == \"Teacher\",\"gender\"] = \"Not Specified\"\n",
    "test.loc[(test['teacher_prefix'] == \"Mrs.\")|(test['teacher_prefix'] == \"Ms.\"),\"gender\"] = \"Female\"\n",
    "\n",
    "lbl = LabelEncoder()\n",
    "test[\"gender\"] = lbl.fit_transform(test[\"gender\"].astype(str))\n",
    "test.fillna(0, inplace=True)\n",
    "\n",
    "#print (test['gender'].unique())\n",
    "\n",
    "#print(test.head())\n",
    "\n",
    "train=train.merge(resources_price,on='id', how='left')\n",
    "train=train.merge(resources_quantity,on='id', how='left')\n",
    "train=train.merge(resources_desc,on='id', how='left')\n",
    "train['project_title_len'] = train['project_title'].apply(lambda x: len(x))\n",
    "train['project_title_wc'] = train['project_title'].apply(lambda x: len(x.split()))\n",
    "#train['project_title_uwc'] = train['project_title'].apply(lambda x: len(set(x.lower().split())))\n",
    "#train['project_title_wc_uwc_ratio'] = train['project_title_uwc'] / train['project_title_wc']\n",
    "train['project_essay_1_len'] = train['project_essay_1'].apply(lambda x: len(x))\n",
    "train['project_essay_1_wc'] = train['project_essay_1'].apply(lambda x: len(x.split()))\n",
    "#train['project_essay_1_uwc'] = train['project_essay_1'].apply(lambda x: len(set(x.lower().split())))\n",
    "#train['project_essay_1_uwc_ratio'] = train['project_essay_1_uwc'] / train['project_essay_1_wc']\n",
    "train['project_essay_2_len'] = train['project_essay_2'].apply(lambda x: len(x))\n",
    "train['project_essay_2_wc'] = train['project_essay_2'].apply(lambda x: len(x.split()))\n",
    "#train['project_essay_2_uwc'] = train['project_essay_2'].apply(lambda x: len(set(x.lower().split())))\n",
    "#train['project_essay_2_uwc_ratio'] = train['project_essay_2_uwc'] / train['project_essay_2_wc']\n",
    "train['project_essay_3_len'] = train['project_essay_3'].apply(lambda x: len(x))\n",
    "train['project_essay_3_wc'] = train['project_essay_3'].apply(lambda x: len(x.split()))\n",
    "#train['project_essay_3_uwc'] = train['project_essay_3'].apply(lambda x: len(set(x.lower().split())))\n",
    "#train['project_essay_3_uwc_ratio'] = train['project_essay_3_uwc'] / train['project_essay_3_wc']\n",
    "train['project_essay_4_len'] = train['project_essay_4'].apply(lambda x: len(x))\n",
    "train['project_essay_4_wc'] = train['project_essay_4'].apply(lambda x: len(x.split()))\n",
    "#train['project_essay_4_uwc'] = train['project_essay_4'].apply(lambda x: len(set(x.lower().split())))\n",
    "#train['project_essay_4_uwc_ratio'] = train['project_essay_4_uwc'] / train['project_essay_4_wc']\n",
    "train['project_resource_summary_len'] = train['project_resource_summary'].apply(lambda x: len(x))\n",
    "train['project_resource_summary_wc'] = train['project_resource_summary'].apply(lambda x: len(x.split()))\n",
    "#train['project_resource_summary_uwc'] = train['project_essay_4'].apply(lambda x: len(set(x.lower().split())))\n",
    "#train['project_resource_summary_uwc_ratio'] = train['project_resource_summary_uwc'] / train['project_resource_summary_wc']\n",
    "train['description_len'] = train['description'].apply(lambda x: len(x))\n",
    "train['description_wc'] = train['description'].apply(lambda x: len(x.split()))\n",
    "train['previous_projects_0'] = train['teacher_number_of_previously_posted_projects'].apply(lambda x: 1 if x == 0 else 0)\n",
    "train['previous_projects_1_5'] = train['teacher_number_of_previously_posted_projects'].apply(lambda x: 1 if 6 > x > 0 else 0)\n",
    "train['previous_projects_6_25'] = train['teacher_number_of_previously_posted_projects'].apply(lambda x: 1 if 26 > x > 5 else 0)\n",
    "train['previous_projects_26_50'] = train['teacher_number_of_previously_posted_projects'].apply(lambda x: 1 if 51 > x > 25 else 0)\n",
    "train['previous_projects_50'] = train['teacher_number_of_previously_posted_projects'].apply(lambda x: 1 if  x > 50 else 0)\n",
    "train['project_submitted_year'] = train['project_submitted_datetime'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).year)\n",
    "train['project_submitted_quarter'] = train['project_submitted_datetime'].apply(lambda x: (datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).month-1)//3 + 1)\n",
    "train['project_submitted_yearmonth'] = train['project_submitted_datetime'].apply(lambda x: int(str(datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).year) + str(datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).month)) )\n",
    "train['project_submitted_month'] = train['project_submitted_datetime'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).month)\n",
    "train['project_submitted_weekday'] = train['project_submitted_datetime'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).weekday())\n",
    "train['project_submitted_hour'] = train['project_submitted_datetime'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).hour)\n",
    "train['project_submitted_season'] = train['project_submitted_datetime'].apply(lambda x: seasons[(datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\" ).month-1)])\n",
    "train[\"gender\"] = None\n",
    "train.loc[train['teacher_prefix'] == \"Mr.\",\"gender\"] = \"Male\"\n",
    "train.loc[train['teacher_prefix'] == \"Teacher\",\"gender\"] = \"Not Specified\"\n",
    "train.loc[(train['teacher_prefix'] == \"Mrs.\")|(train['teacher_prefix'] == \"Ms.\"),\"gender\"] = \"Female\"\n",
    "train[\"gender\"] = lbl.fit_transform(train[\"gender\"].astype(str))\n",
    "train.fillna(0, inplace=True)\n",
    "\n",
    "#print(train.head())\n",
    "\n",
    "#train.loc[train['teacher_number_of_previously_posted_projects'].idxmax()]\n",
    "#train.hist(column='teacher_number_of_previously_posted_projects')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystopwords = (stopwords.words('english')) + list(stop_words.ENGLISH_STOP_WORDS) \n",
    "#count_vect = CountVectorizer(stop_words=mystopwords,analyzer='word',token_pattern='[^\\W\\d_]{2,}',\\\n",
    "#                             ngram_range=(1,1), max_features=800000, min_df=2,strip_accents='unicode')\n",
    "#\n",
    "texts = train['project_essay_1'].tolist() + train['project_essay_2'].tolist() \\\n",
    "    + train['project_essay_3'].tolist() + train['project_essay_4'].tolist() \\\n",
    "    + train['project_resource_summary'].tolist() + train['project_title'].tolist() + train['description'].tolist() \n",
    "    \n",
    "train['project_essay'] = train.apply(lambda row: ' '.join([\n",
    "    str(row['project_essay_1']), \n",
    "    str(row['project_essay_2']), \n",
    "    str(row['project_essay_3']), \n",
    "    str(row['project_essay_4']),\n",
    "    ]), axis=1)\n",
    "test['project_essay'] = test.apply(lambda row: ' '.join([ \n",
    "    str(row['project_essay_1']), \n",
    "    str(row['project_essay_2']), \n",
    "    str(row['project_essay_3']), \n",
    "    str(row['project_essay_4']),\n",
    "    ]), axis=1)\n",
    " \n",
    "#stemmer = PorterStemmer()\n",
    "\n",
    "#train['project_essay'] = train['project_essay'].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split(\" \")]))\n",
    "#test['project_essay'] = test['project_essay'].apply(lambda x: \" \".join([stemmer.stem(word) for word in x.split(\" \")]))\n",
    "    \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "print('Preprocessing text...')\n",
    "cols = [\n",
    "    'project_essay',\n",
    "    'description',\n",
    "    #'project_essay_2', \n",
    "    #'project_essay_3',\n",
    "    #'project_essay_4',\n",
    "    'project_title',\n",
    "    'project_resource_summary'\n",
    "]\n",
    "\n",
    "n_features = [\n",
    "    4000,\n",
    "    400,\n",
    "    400,\n",
    "    400\n",
    "]\n",
    "\n",
    "for c_i, c in enumerate(cols):\n",
    "    print ('Processing col: ' + c)\n",
    "    tfidf = TfidfVectorizer(max_features=n_features[c_i], min_df=4,stop_words=mystopwords,analyzer='word',\\\n",
    "                            token_pattern='[^\\W\\d_]{2,}', ngram_range=(1,1),strip_accents='unicode')\n",
    "    tfidf.fit(texts)\n",
    "    \n",
    "    tfidf_train = np.array(tfidf.transform(train[c]).todense(), dtype=np.float16)\n",
    "    tfidf_test = np.array(tfidf.transform(test[c]).todense(), dtype=np.float16)\n",
    "\n",
    "    for i in range(n_features[c_i]):\n",
    "        train[c + '_tfidf_' + str(i)] = tfidf_train[:, i]\n",
    "        test[c + '_tfidf_' + str(i)] = tfidf_test[:, i]\n",
    "        \n",
    "    del tfidf, tfidf_train, tfidf_test    \n",
    "    \n",
    "#train.head()\n",
    "\n",
    "'''X_train_counts = count_vect.fit_transform(texts)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "weights = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': count_vect.get_feature_names(), 'weight': weights})\n",
    "weights_df = weights_df.sort_values(ascending=False, by=['weight'])\n",
    "weights_df.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('performing sentiment analysis')\n",
    "\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "SIA = SentimentIntensityAnalyzer()\n",
    "\n",
    "for col in cols:\n",
    "    print ('analyzing col ' + col)\n",
    "    train[col+\"_vader_Compound\"]= train[col].apply(lambda x:SIA.polarity_scores(x)['compound'])\n",
    "    train[col+'_vader_Negative']= train[col].apply(lambda x:SIA.polarity_scores(x)['neg'])\n",
    "    train[col+'_vader_Positive']= train[col].apply(lambda x:SIA.polarity_scores(x)['pos'])\n",
    "    test[col+\"_vader_Compound\"]= test[col].apply(lambda x:SIA.polarity_scores(x)['compound'])\n",
    "    test[col+'_vader_Negative']= test[col].apply(lambda x:SIA.polarity_scores(x)['neg'])\n",
    "    test[col+'_vader_Positive']= test[col].apply(lambda x:SIA.polarity_scores(x)['pos'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=mystopwords,analyzer='word',token_pattern='[^\\W\\d_]{2,}',\\\n",
    "                             ngram_range=(1,1), max_features=4000, min_df=4,strip_accents='unicode')\n",
    "X_train_counts = count_vect.fit_transform(texts)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "weights = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': count_vect.get_feature_names(), 'weight': weights})\n",
    "weights_df = weights_df.sort_values(ascending=False, by=['weight'])\n",
    "weights_df.head()\n",
    "weights_df.to_csv('weights_df.csv', index=False,quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = train['project_subject_categories'].unique()\n",
    "subcategories = train['project_subject_subcategories'].unique()\n",
    "\n",
    "cat_set = set()\n",
    "for cat in categories:\n",
    "    for c in cat.split(','):\n",
    "        cat_set.add(c.strip())\n",
    "#print(cat_set)\n",
    "for cat in cat_set:\n",
    "    train['project_subject_categories_' + cat] =  train['project_subject_categories'].apply(lambda x : 1 if cat in x else 0)\n",
    "    test['project_subject_categories_' + cat] =  test['project_subject_categories'].apply(lambda x : 1 if cat in x else 0)\n",
    "    \n",
    "sub_cat_set = set()\n",
    "for cat in subcategories:\n",
    "    for c in cat.split(','):\n",
    "        sub_cat_set.add(c.strip())\n",
    "#print(sub_cat_set)\n",
    "for cat in sub_cat_set:\n",
    "    train['project_subject_subcategories_' + cat] =  train['project_subject_subcategories'].apply(lambda x : 1 if cat in x else 0)\n",
    "    test['project_subject_subcategories_' + cat] =  test['project_subject_subcategories'].apply(lambda x : 1 if cat in x else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['project_is_approved']\n",
    "\n",
    "train['school_state'] = 'state_' + train['school_state'].astype(str)  \n",
    "train['teacher_prefix'] = 'teacher_prefix_' + train['teacher_prefix'].astype(str)\n",
    "train['project_grade_category'] = 'project_grade_category' + train['project_grade_category'].astype(str)\n",
    "train['project_subject_categories'] = 'project_subject_categories' + train['project_subject_categories'].astype(str)\n",
    "#train['project_subject_subcategories'] = 'project_subject_subcategories' + train['project_subject_subcategories'].astype(str)\n",
    "\n",
    "train = train.join(pd.get_dummies(train['school_state']))\n",
    "train = train.join(pd.get_dummies(train['teacher_prefix']))\n",
    "train = train.join(pd.get_dummies(train['project_grade_category']))\n",
    "train = train.join(pd.get_dummies(train['project_subject_categories']))\n",
    "#train = train.join(pd.get_dummies(train['project_subject_subcategories']))\n",
    "\n",
    "train2 = train.drop(['teacher_prefix_Mr.','state_NV', 'project_grade_categoryGrades PreK-2',\\\n",
    "                     'project_subject_categoriesLiteracy & Language','project_subject_subcategoriesLiteracy'], errors='ignore', axis=1)  \n",
    "train2 = train2.drop(['id','teacher_prefix','school_state','project_grade_category',\\\n",
    "                      'project_subject_subcategories', 'project_subject_categories','project_title',\"project_essay_1\",\\\n",
    "                \"project_essay_2\",\"project_essay\",\"project_essay_3\",\"project_essay_4\",\\\n",
    "                      \"project_essay\",\"project_resource_summary\",\\\n",
    "                      'project_submitted_datetime','description','count_x','count_y'], errors='ignore',axis=1)  \n",
    "train2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ((train2.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['school_state'] = 'state_' + test['school_state'].astype(str)  \n",
    "test['teacher_prefix'] = 'teacher_prefix_' + test['teacher_prefix'].astype(str)\n",
    "test['project_grade_category'] = 'project_grade_category' + test['project_grade_category'].astype(str)\n",
    "test['project_subject_categories'] = 'project_subject_categories' + test['project_subject_categories'].astype(str)\n",
    "#test['project_subject_subcategories'] = 'project_subject_subcategories' + test['project_subject_subcategories'].astype(str)\n",
    "\n",
    "test = test.join(pd.get_dummies(test['school_state']))\n",
    "test = test.join(pd.get_dummies(test['teacher_prefix']))\n",
    "test = test.join(pd.get_dummies(test['project_grade_category']))\n",
    "test = test.join(pd.get_dummies(test['project_subject_categories']))\n",
    "#test = test.join(pd.get_dummies(test['project_subject_subcategories']))\n",
    "\n",
    "test2 = test.drop(['teacher_prefix_Mr.','state_NV', 'project_grade_categoryGrades PreK-2', \\\n",
    "                   'project_subject_categoriesLiteracy & Language','project_subject_subcategoriesLiteracy'], axis=1, errors='ignore')  \n",
    "test2 = test2.drop(['id','teacher_prefix','school_state','project_grade_category',\\\n",
    "                    'project_subject_subcategories','project_subject_categories','project_title',\"project_essay_1\",\\\n",
    "                \"project_essay_2\",\"project_essay_3\",\"project_essay_4\",\"project_essay\", \"project_resource_summary\",\\\n",
    "                'project_submitted_datetime','description','count_x','count_y'],axis=1, errors='ignore')\n",
    "\n",
    "test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainn, test, validate = np.split(train.sample(frac=1), [int(.6*len(train)), int(.8*len(train))]) \n",
    "\n",
    "#print (trainn.shape, test.shape, validate.shape)\n",
    "\n",
    "y_train = train2['project_is_approved']  \n",
    "x_train = train2.drop(['project_is_approved','teacher_prefix','school_state','project_grade_category',\\\n",
    "                       'project_subject_subcategories','project_subject_categories','project_title'\"project_essay_1\",\\\n",
    "                \"project_essay_2\",\"project_essay_3\",\"project_essay_4\",\"project_resource_summary\"], errors='ignore',axis=1)  \n",
    "y_test = train[target_cols]  \n",
    "#x_test = test.drop(['project_is_approved','teacher_prefix','school_state','project_grade_category','project_subject_categories'], axis=1)  \n",
    "\n",
    "#x_testt = testt[feature_cols]\n",
    "\n",
    "\n",
    "'''print (test2.shape)\n",
    "print (y_test.shape)\n",
    "\n",
    "rf = RandomForestClassifier()  \n",
    "rf.fit(x_train, y_train)  \n",
    "print (rf.score(x_train, y_train)) \n",
    "print (\"Features sorted by their score:\" ) \n",
    "print (sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), x_train), reverse=True)  )\n",
    "\n",
    "x_train.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/Users/schwalmdaniel/github/xgboost/python-package\")\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "#model = XGBClassifier()\n",
    "#model.fit(x_train, y_train)\n",
    "#print (model.score(x_train, y_train))\n",
    "\n",
    "logit_model = XGBClassifier() #LogisticRegression()  \n",
    "# Fit\n",
    "logit_model = logit_model.fit(x_train, y_train,eval_metric='auc')  \n",
    "# How accurate?\n",
    "print (logit_model.score(x_train, y_train))  \n",
    "#0.7874\n",
    "\n",
    "# How does it perform on the test dataset?\n",
    "\n",
    "# Predictions on the test dataset\n",
    "predicted = pd.DataFrame(logit_model.predict(test2))  \n",
    "# Probabilities on the test dataset\n",
    "probs = pd.DataFrame(logit_model.predict_proba(test2))  \n",
    "#prd[:,i] = np.round(probs,5)[:,1]\n",
    "#print (metrics.accuracy_score(y_test, predicted)  )\n",
    "\n",
    "print (probs.shape)\n",
    "\n",
    "prd_1 = pd.DataFrame(probs)\n",
    "\n",
    "import csv\n",
    "\n",
    "submit = pd.concat([test['id'],prd_1],axis=1)\n",
    "\n",
    "print (submit.columns.tolist)\n",
    "\n",
    "submit = submit.drop(submit.columns[1], axis=1)\n",
    "#probs.head()\n",
    "submit.to_csv('donor1.csv',index=False,quoting=csv.QUOTE_NONNUMERIC)\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "\n",
    "import gc\n",
    "import csv\n",
    "\n",
    "cnt = 0\n",
    "p_buf = []\n",
    "n_splits = 2\n",
    "n_repeats = 1\n",
    "kf = RepeatedKFold(\n",
    "    n_splits=n_splits, \n",
    "    n_repeats=n_repeats, \n",
    "    random_state=0)\n",
    "auc_buf = []   \n",
    "\n",
    "params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'max_depth': 12,\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.025,\n",
    "        'feature_fraction': 0.85,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0,\n",
    "        'num_threads': 4,\n",
    "        'lambda_l2': 1.5,\n",
    "        'min_gain_to_split': 0,\n",
    "    }  \n",
    "\n",
    "for train_index, valid_index in kf.split(x_train):\n",
    "    print('Fold {}/{}'.format(cnt + 1, n_splits))\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb.Dataset(x_train.loc[train_index], y_train.loc[train_index], feature_name=x_train.columns.tolist()),\n",
    "        num_boost_round=10000,\n",
    "        valid_sets=[lgb.Dataset(x_train.loc[valid_index], y_train.loc[valid_index])],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=100,\n",
    "    )\n",
    "\n",
    "    if cnt == 0:\n",
    "        importance = model.feature_importance()\n",
    "        model_fnames = model.feature_name()\n",
    "        tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]\n",
    "        tuples = [x for x in tuples if x[1] > 0]\n",
    "        print('Important features:')\n",
    "        print(tuples[:200])\n",
    "\n",
    "    p = model.predict(x_train.loc[valid_index], num_iteration=model.best_iteration)\n",
    "    auc = roc_auc_score(y_train.loc[valid_index], p)\n",
    "\n",
    "    print('{} AUC: {}'.format(cnt, auc))\n",
    "\n",
    "    p = model.predict(test2, num_iteration=model.best_iteration)\n",
    "    if len(p_buf) == 0:\n",
    "        p_buf = np.array(p)\n",
    "    else:\n",
    "        p_buf += np.array(p)\n",
    "    auc_buf.append(auc)\n",
    "\n",
    "    cnt += 1\n",
    "    if cnt > 0: # Comment this to run several folds\n",
    "        break\n",
    "    \n",
    "    del model\n",
    "    gc.collect\n",
    "\n",
    "auc_mean = np.mean(auc_buf)\n",
    "auc_std = np.std(auc_buf)\n",
    "print('AUC = {:.6f} +/- {:.6f}'.format(auc_mean, auc_std))\n",
    "\n",
    "preds = p_buf/cnt\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['id'] = test['id']\n",
    "subm['project_is_approved'] = preds\n",
    "subm.to_csv('talkingdata1.csv', index=False,quoting=csv.QUOTE_NONNUMERIC)\n",
    "subm.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.hist('previous_projects_50')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate with a max depth of 3\n",
    "tree_model = tree.DecisionTreeClassifier(max_depth=3)  \n",
    "# Fit a decision tree\n",
    "tree_model = tree_model.fit(x_train, y_train)  \n",
    "# Training accuracy\n",
    "tree_model.score(x_train, y_train)\n",
    "\n",
    "# Predictions/probs on the test dataset\n",
    "predicted = pd.DataFrame(tree_model.predict(x_test))  \n",
    "probs = pd.DataFrame(tree_model.predict_proba(x_test))\n",
    "\n",
    "# Store metrics\n",
    "tree_accuracy = metrics.accuracy_score(y_test, predicted)  \n",
    "tree_roc_auc = metrics.roc_auc_score(y_test, probs[1])  \n",
    "tree_confus_matrix = metrics.confusion_matrix(y_test, predicted)  \n",
    "tree_classification_report = metrics.classification_report(y_test, predicted)  \n",
    "tree_precision = metrics.precision_score(y_test, predicted, pos_label=1)  \n",
    "tree_recall = metrics.recall_score(y_test, predicted, pos_label=1)  \n",
    "tree_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n",
    "\n",
    "print (tree_accuracy)\n",
    "\n",
    "# evaluate the model using 10-fold cross-validation\n",
    "tree_cv_scores = cross_val_score(tree.DecisionTreeClassifier(max_depth=3), x_test, y_test, scoring='precision', cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "rf = RandomForestClassifier()  \n",
    "# Fit\n",
    "rf_model = rf.fit(x_train, y_train)  \n",
    "# training accuracy 99.74%\n",
    "rf_model.score(x_train, y_train)\n",
    "\n",
    "# Predictions/probs on the test dataset\n",
    "predicted = pd.DataFrame(rf_model.predict(x_test))  \n",
    "probs = pd.DataFrame(rf_model.predict_proba(x_test))\n",
    "\n",
    "# Store metrics\n",
    "rf_accuracy = metrics.accuracy_score(y_test, predicted)  \n",
    "rf_roc_auc = metrics.roc_auc_score(y_test, probs[1])  \n",
    "rf_confus_matrix = metrics.confusion_matrix(y_test, predicted)  \n",
    "rf_classification_report = metrics.classification_report(y_test, predicted)  \n",
    "rf_precision = metrics.precision_score(y_test, predicted, pos_label=1)  \n",
    "rf_recall = metrics.recall_score(y_test, predicted, pos_label=1)  \n",
    "rf_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n",
    "\n",
    "print (rf_accuracy)\n",
    "\n",
    "# Evaluate the model using 10-fold cross-validation\n",
    "#rf_cv_scores = cross_val_score(RandomForestClassifier(), x_test, y_test, scoring='precision', cv=10)  \n",
    "#rf_cv_mean = np.mean(rf_cv_scores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "svm_model = SVC(probability=True)  \n",
    "# Fit\n",
    "svm_model = svm_model.fit(x_train, y_train)  \n",
    "# Accuracy\n",
    "svm_model.score(x_train, y_train)\n",
    "\n",
    "# Predictions/probs on the test dataset\n",
    "predicted = pd.DataFrame(svm_model.predict(x_test))  \n",
    "probs = pd.DataFrame(svm_model.predict_proba(x_test))\n",
    "\n",
    "# Store metrics\n",
    "svm_accuracy = metrics.accuracy_score(y_test, predicted)  \n",
    "svm_roc_auc = metrics.roc_auc_score(y_test, probs[1])  \n",
    "svm_confus_matrix = metrics.confusion_matrix(y_test, predicted)  \n",
    "svm_classification_report = metrics.classification_report(y_test, predicted)  \n",
    "svm_precision = metrics.precision_score(y_test, predicted, pos_label=1)  \n",
    "svm_recall = metrics.recall_score(y_test, predicted, pos_label=1)  \n",
    "svm_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n",
    "\n",
    "print (svm_accuracy)\n",
    "\n",
    "# Evaluate the model using 10-fold cross-validation\n",
    "#svm_cv_scores = cross_val_score(SVC(probability=True), x_test, y_test, scoring='precision', cv=10)  \n",
    "#svm_cv_mean = np.mean(svm_cv_scores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate learning model (k = 3)\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)  \n",
    "# fit the model\n",
    "knn_model.fit(x_train, y_train)  \n",
    "# Accuracy\n",
    "knn_model.score(x_train, y_train)\n",
    "\n",
    "# Predictions/probs on the test dataset\n",
    "predicted = pd.DataFrame(knn_model.predict(x_test))  \n",
    "probs = pd.DataFrame(knn_model.predict_proba(x_test))\n",
    "\n",
    "# Store metrics\n",
    "knn_accuracy = metrics.accuracy_score(y_test, predicted)  \n",
    "knn_roc_auc = metrics.roc_auc_score(y_test, probs[1])  \n",
    "knn_confus_matrix = metrics.confusion_matrix(y_test, predicted)  \n",
    "knn_classification_report = metrics.classification_report(y_test, predicted)  \n",
    "knn_precision = metrics.precision_score(y_test, predicted, pos_label=1)  \n",
    "knn_recall = metrics.recall_score(y_test, predicted, pos_label=1)  \n",
    "knn_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n",
    "\n",
    "# Evaluate the model using 10-fold cross-validation\n",
    "#knn_cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors=3), x_test, y_test, scoring='precision', cv=10)  \n",
    "#knn_cv_mean = np.mean(knn_cv_scores)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate\n",
    "bayes_model = GaussianNB()  \n",
    "# Fit the model\n",
    "bayes_model.fit(x_train, y_train)  \n",
    "# Accuracy\n",
    "bayes_model.score(x_train, y_train)\n",
    "\n",
    "# Predictions/probs on the test dataset\n",
    "predicted = pd.DataFrame(bayes_model.predict(x_test))  \n",
    "probs = pd.DataFrame(bayes_model.predict_proba(x_test))\n",
    "\n",
    "# Store metrics\n",
    "bayes_accuracy = metrics.accuracy_score(y_test, predicted)  \n",
    "bayes_roc_auc = metrics.roc_auc_score(y_test, probs[1])  \n",
    "bayes_confus_matrix = metrics.confusion_matrix(y_test, predicted)  \n",
    "bayes_classification_report = metrics.classification_report(y_test, predicted)  \n",
    "bayes_precision = metrics.precision_score(y_test, predicted, pos_label=1)  \n",
    "bayes_recall = metrics.recall_score(y_test, predicted, pos_label=1)  \n",
    "bayes_f1 = metrics.f1_score(y_test, predicted, pos_label=1)\n",
    "\n",
    "print (rf_accuracy)\n",
    "\n",
    "# Evaluate the model using 10-fold cross-validation\n",
    "#bayes_cv_scores = cross_val_score(KNeighborsClassifier(n_neighbors=3), x_test, y_test, scoring='precision', cv=10)  \n",
    "#bayes_cv_mean = np.mean(bayes_cv_scores)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
