{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import re\n",
    "import gc\n",
    "import sys, os, random\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(37)\n",
    "random.seed(17)\n",
    "\n",
    "pd.set_option('display.max_rows',1000)\n",
    "\n",
    "root = '/Users/schwalmdaniel/github/kaggle/home-credit-default-risk'\n",
    "#root = 'e:/kaggle/home-credit-default-risk'\n",
    "\n",
    "train=pd.read_csv(root + \"/application_train.csv\")\n",
    "test=pd.read_csv(root + \"/application_test.csv\")\n",
    "bureau=pd.read_csv(root + \"/bureau.csv\")\n",
    "previous_application=pd.read_csv(root + \"/previous_application.csv\")\n",
    "installments_payments=pd.read_csv(root + \"/installments_payments.csv\")\n",
    "bureau_balance=pd.read_csv(root + \"/bureau_balance.csv\")\n",
    "POS_CASH_balance=pd.read_csv(root + \"/POS_CASH_balance.csv\")\n",
    "credit_card_balance=pd.read_csv(root + \"/credit_card_balance.csv\")\n",
    "\n",
    "\n",
    "# have a look at the ds\n",
    "#train.head()\n",
    "\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_CASH_balance.head()\n",
    "\n",
    "num_aggregations = {\n",
    "        'CNT_INSTALMENT': ['min', 'max', 'mean','sum','var'],\n",
    "        'CNT_INSTALMENT_FUTURE': ['min', 'max', 'mean','sum','var'],\n",
    "        'SK_DPD': ['min', 'max', 'mean'],\n",
    "        'SK_DPD_DEF': ['min', 'max', 'mean'],\n",
    "    }\n",
    "\n",
    "POS_CASH_balance_cat, cat_cols = one_hot_encoder(POS_CASH_balance, nan_as_category= True)\n",
    "cat_aggregations = {}\n",
    "    \n",
    "for cat in cat_cols:\n",
    "    cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "POS_CASH_balance_agg = POS_CASH_balance_cat.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "POS_CASH_balance_agg.columns = pd.Index(['POSBAL_' + e[0] + \"_\" + e[1].upper() for e in POS_CASH_balance_agg.columns.tolist()])\n",
    "\n",
    "train=train.join(POS_CASH_balance_agg,on='SK_ID_CURR', how='left')\n",
    "test=test.join(POS_CASH_balance_agg,on='SK_ID_CURR', how='left')\n",
    "\n",
    "del POS_CASH_balance_agg,POS_CASH_balance_cat"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# without bureau balance it was 0.771, now it is 0.768. check if there is something TODO\n",
    "\n",
    "bureau_bal_joined  =bureau.merge(bureau_balance,on='SK_ID_BUREAU', how='left')\n",
    "bureau_bal_joined_red = bureau_bal_joined[['SK_ID_CURR','MONTHS_BALANCE','STATUS']]\n",
    "bureau_bal_joined_red['STATUS'].fillna('X',inplace=True)\n",
    "balance_mean = bureau_bal_joined_red['MONTHS_BALANCE'].mean()\n",
    "bureau_bal_joined_red['MONTHS_BALANCE'].fillna(balance_mean,inplace=True)\n",
    "\n",
    "bureau_bal_joined_red_cat, cat_cols = one_hot_encoder(bureau_bal_joined_red, nan_as_category= True)\n",
    "cat_aggregations = {}\n",
    "    \n",
    "for cat in cat_cols:\n",
    "    cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "bureau_bal_joined_agg = bureau_bal_joined_red_cat.groupby('SK_ID_CURR').agg({**cat_aggregations})\n",
    "bureau_bal_joined_agg.columns = pd.Index(['BURBAL_' + e[0] + \"_\" + e[1].upper() for e in bureau_bal_joined_agg.columns.tolist()])\n",
    "\n",
    "train=train.join(bureau_bal_joined_agg,on='SK_ID_CURR', how='left')\n",
    "test=test.join(bureau_bal_joined_agg,on='SK_ID_CURR', how='left')\n",
    "\n",
    "del bureau_bal_joined_agg,bureau_bal_joined,bureau_bal_joined_red,bureau_bal_joined_red_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_card_balance.head()\n",
    "\n",
    "num_aggregations = {\n",
    "        'AMT_BALANCE': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT_LIMIT_ACTUAL': ['min', 'max', 'mean'],\n",
    "        'AMT_DRAWINGS_ATM_CURRENT': ['min', 'max', 'mean'],\n",
    "        'AMT_DRAWINGS_CURRENT': ['min', 'max', 'mean'],\n",
    "        'AMT_DRAWINGS_OTHER_CURRENT': ['min', 'max', 'mean'],\n",
    "        'AMT_DRAWINGS_POS_CURRENT': ['min', 'max', 'mean'],\n",
    "        'AMT_INST_MIN_REGULARITY': ['min', 'max', 'mean'],\n",
    "        'AMT_PAYMENT_CURRENT': ['min', 'max', 'mean'],\n",
    "        'AMT_PAYMENT_TOTAL_CURRENT': ['min', 'max', 'mean'],\n",
    "        'AMT_RECEIVABLE_PRINCIPAL': ['min', 'max', 'mean'],\n",
    "        'AMT_RECIVABLE': ['min', 'max', 'mean'],\n",
    "        'AMT_TOTAL_RECEIVABLE': ['min', 'max', 'mean'],\n",
    "        'CNT_DRAWINGS_ATM_CURRENT': ['min', 'max', 'mean','sum','var'],\n",
    "        'CNT_DRAWINGS_CURRENT': ['min', 'max', 'mean','sum','var'],\n",
    "        'CNT_DRAWINGS_OTHER_CURRENT': ['min', 'max', 'mean','sum','var'],\n",
    "        'CNT_DRAWINGS_POS_CURRENT': ['min', 'max', 'mean','sum','var'],\n",
    "        'CNT_INSTALMENT_MATURE_CUM': ['min', 'max', 'mean','sum','var'],\n",
    "        'SK_DPD': ['min', 'max', 'mean'],\n",
    "        'SK_DPD_DEF': ['min', 'max', 'mean'],\n",
    "    }\n",
    "\n",
    "credit_card_balance_cat, cat_cols = one_hot_encoder(credit_card_balance, nan_as_category= True)\n",
    "cat_aggregations = {}\n",
    "    \n",
    "for cat in cat_cols:\n",
    "    cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "credit_card_balance_agg = credit_card_balance_cat.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "credit_card_balance_agg.columns = pd.Index(['CREBAL_' + e[0] + \"_\" + e[1].upper() for e in credit_card_balance_agg.columns.tolist()])\n",
    "\n",
    "train=train.join(credit_card_balance_agg,on='SK_ID_CURR', how='left')\n",
    "test=test.join(credit_card_balance_agg,on='SK_ID_CURR', how='left')\n",
    "\n",
    "del credit_card_balance_agg,credit_card_balance_cat"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "previous_application['CODE_REJECT_REASON'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_app_rejected = previous_application[(previous_application['CODE_REJECT_REASON'] != 'XAP')]\\\n",
    "    .groupby('SK_ID_CURR').CODE_REJECT_REASON.agg(['count']).reset_index()\n",
    "train=train.merge(prev_app_rejected,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(prev_app_rejected,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'count': 'prev_app_rejected_count'}, inplace=True)\n",
    "test.rename(columns={'count': 'prev_app_rejected_count'}, inplace=True)\n",
    "train['prev_app_rejected_count'].fillna(0,inplace=True)\n",
    "test['prev_app_rejected_count'].fillna(0,inplace=True)\n",
    "\n",
    "prev_app_applied_diff = previous_application[['SK_ID_CURR','AMT_APPLICATION','AMT_CREDIT']]\n",
    "prev_app_applied_diff['CREDIT_APPL_DIFF'] =  prev_app_applied_diff['AMT_CREDIT'] - \\\n",
    "    prev_app_applied_diff['AMT_APPLICATION'] \n",
    "\n",
    "prev_app_credit_diff = prev_app_applied_diff.groupby('SK_ID_CURR').\\\n",
    "    CREDIT_APPL_DIFF.agg(['min']).reset_index()\n",
    "train=train.merge(prev_app_credit_diff,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(prev_app_credit_diff,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'min': 'prev_app_credit_diff_min'}, inplace=True)\n",
    "                      #,'max': 'prev_app_credit_diff_max','mean': 'prev_app_credit_diff_mean'}\n",
    "                     \n",
    "test.rename(columns={'min': 'prev_app_credit_diff_min'}, inplace=True)\n",
    "                      #,'max': 'prev_app_credit_diff_max','mean': 'prev_app_credit_diff_mean'}\n",
    "                     #, inplace=True})\n",
    "train['prev_app_credit_diff_min'].fillna(0,inplace=True)\n",
    "#train['prev_app_credit_diff_max'].fillna(0,inplace=True)\n",
    "#train['prev_app_credit_diff_mean'].fillna(0,inplace=True)\n",
    "test['prev_app_credit_diff_min'].fillna(0,inplace=True)\n",
    "#test['prev_app_credit_diff_max'].fillna(0,inplace=True)\n",
    "#test['prev_app_credit_diff_mean'].fillna(0,inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum','var'],\n",
    "    }\n",
    "\n",
    "prev_app_cat, cat_cols = one_hot_encoder(previous_application, nan_as_category= True)\n",
    "cat_aggregations = {}\n",
    "    \n",
    "for cat in cat_cols:\n",
    "    cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "prev_agg = prev_app_cat.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "\n",
    "train=train.join(prev_agg,on='SK_ID_CURR', how='left')\n",
    "test=test.join(prev_agg,on='SK_ID_CURR', how='left')\n",
    "\n",
    "del prev_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_aggregations = {\n",
    "        'DAYS_PAYMENT_DIFF': ['min', 'max', 'mean','sum'],\n",
    "        'PAYMENT_DIFF': ['min', 'max', 'mean','sum','var'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum','var'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_RATIO': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "    }\n",
    "\n",
    "ins_pay = installments_payments.drop(['SK_ID_PREV','NUM_INSTALMENT_VERSION','NUM_INSTALMENT_NUMBER'],axis=1)\n",
    "ins_pay['DAYS_PAYMENT_DIFF'] = ins_pay['DAYS_ENTRY_PAYMENT'] - ins_pay['DAYS_INSTALMENT']\n",
    "ins_pay['PAYMENT_DIFF'] = ins_pay['AMT_INSTALMENT'] - ins_pay['AMT_PAYMENT']\n",
    "ins_pay['DBD'] = ins_pay['DAYS_INSTALMENT'] - ins_pay['DAYS_ENTRY_PAYMENT']\n",
    "ins_pay['DAYS_PAYMENT_DIFF'] = ins_pay['DAYS_PAYMENT_DIFF'].apply(lambda x: x if x > 0 else 0)\n",
    "ins_pay['DBD'] = ins_pay['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "ins_pay['PAYMENT_RATIO'] = ins_pay['AMT_PAYMENT'] / ins_pay['AMT_INSTALMENT']\n",
    "\n",
    "ins_pay_agg = ins_pay.groupby('SK_ID_CURR').agg({**num_aggregations})\n",
    "ins_pay_agg.columns = pd.Index(['INS_' + e[0] + \"_\" + e[1].upper() for e in ins_pay_agg.columns.tolist()])\n",
    "\n",
    "ins_pay_agg.head()\n",
    "train=train.join(ins_pay_agg,on='SK_ID_CURR', how='left')\n",
    "test=test.join(ins_pay_agg,on='SK_ID_CURR', how='left')\n",
    "\n",
    "del ins_pay_agg, ins_pay"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureaue_active_credit_limit = bureau[(bureau['CREDIT_ACTIVE'] == 'Active') & (bureau['AMT_CREDIT_SUM_LIMIT'].notnull())\\\n",
    "            & (bureau['AMT_CREDIT_SUM_LIMIT'] > 0.0)]\\\n",
    "    .groupby('SK_ID_CURR').AMT_CREDIT_SUM_LIMIT.agg(['sum']).reset_index()\n",
    "\n",
    "bureau_last_credit_update_active = bureau[(bureau['CREDIT_ACTIVE'] == 'Active')]\\\n",
    "    .groupby('SK_ID_CURR').DAYS_CREDIT_UPDATE.agg(['min','max']).reset_index()\n",
    "bureau_last_credit_update_closed = bureau[(bureau['CREDIT_ACTIVE'] == 'Closed')]\\\n",
    "    .groupby('SK_ID_CURR').DAYS_CREDIT_UPDATE.agg(['min','max']).reset_index()\n",
    "bureau_last_credit_update = bureau.groupby('SK_ID_CURR').DAYS_CREDIT_UPDATE.agg(['min','max']).reset_index()\n",
    "\n",
    "train=train.merge(bureau_last_credit_update_active,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_last_credit_update_active,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'max': 'bureau_last_credit_update_active_max',\n",
    "                      'min': 'bureau_last_credit_update_active_min'}, inplace=True)\n",
    "test.rename(columns={'max': 'bureau_last_credit_update_active_max',\n",
    "                     'min': 'bureau_last_credit_update_active_min'}, inplace=True)\n",
    "train['bureau_last_credit_update_active_max'].fillna(0,inplace=True)\n",
    "train['bureau_last_credit_update_active_min'].fillna(0,inplace=True)\n",
    "test['bureau_last_credit_update_active_max'].fillna(0,inplace=True)\n",
    "test['bureau_last_credit_update_active_min'].fillna(0,inplace=True)\n",
    "\n",
    "train=train.merge(bureau_last_credit_update_closed,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_last_credit_update_closed,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'max': 'bureau_last_credit_update_closed_max',\n",
    "                      'min': 'bureau_last_credit_update_closed_min'}, inplace=True)\n",
    "test.rename(columns={'max': 'bureau_last_credit_update_closed_max',\n",
    "                     'min': 'bureau_last_credit_update_closed_min'}, inplace=True)\n",
    "train['bureau_last_credit_update_closed_max'].fillna(0,inplace=True)\n",
    "train['bureau_last_credit_update_closed_min'].fillna(0,inplace=True)\n",
    "test['bureau_last_credit_update_closed_max'].fillna(0,inplace=True)\n",
    "test['bureau_last_credit_update_closed_min'].fillna(0,inplace=True)\n",
    "\n",
    "train=train.merge(bureau_last_credit_update,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_last_credit_update,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'max': 'bureau_last_credit_update_max',\n",
    "                      'min': 'bureau_last_credit_update_min'}, inplace=True)\n",
    "test.rename(columns={'max': 'bureau_last_credit_update_max',\n",
    "                     'min': 'bureau_last_credit_update_min'}, inplace=True)\n",
    "train['bureau_last_credit_update_max'].fillna(0,inplace=True)\n",
    "train['bureau_last_credit_update_min'].fillna(0,inplace=True)\n",
    "test['bureau_last_credit_update_max'].fillna(0,inplace=True)\n",
    "test['bureau_last_credit_update_min'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ax1 = sns.distplot(train[\"WEEKDAY_APPR_PROCESS_START\"][train.TARGET==1], color='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_credit = pd.DataFrame(bureau[['SK_ID_CURR', 'CREDIT_ACTIVE']])\n",
    "\n",
    "bureau_overdue = pd.DataFrame(bureau[['SK_ID_CURR', 'CREDIT_DAY_OVERDUE']])\n",
    "bureau_credit_type = pd.DataFrame(bureau[['SK_ID_CURR', 'CREDIT_ACTIVE','CREDIT_TYPE']])\n",
    "bureau_sum_debt = pd.DataFrame(bureau[['SK_ID_CURR', 'CREDIT_ACTIVE','AMT_CREDIT_SUM_DEBT']])\n",
    "\n",
    "bureau_credit_active = bureau_credit[bureau_credit['CREDIT_ACTIVE'] == 'Active'].\\\n",
    "    groupby('SK_ID_CURR').CREDIT_ACTIVE.agg(['count']).reset_index()\n",
    "bureau_credit_closed = bureau_credit[bureau_credit['CREDIT_ACTIVE'] == 'Closed'].\\\n",
    "    groupby('SK_ID_CURR').CREDIT_ACTIVE.agg(['count']).reset_index()\n",
    "bureau_credit_type_active = bureau_credit_type[bureau_credit_type['CREDIT_ACTIVE'] == 'Active'].\\\n",
    "    groupby('SK_ID_CURR').CREDIT_TYPE.agg(['count']).reset_index()\n",
    "bureau_credit_type_closed = bureau_credit_type[bureau_credit_type['CREDIT_ACTIVE'] == 'Closed'].\\\n",
    "    groupby('SK_ID_CURR').CREDIT_TYPE.agg(['count']).reset_index()\n",
    "bureau_sum_debt_active = bureau_sum_debt[bureau_sum_debt['CREDIT_ACTIVE'] == 'Active'].\\\n",
    "    groupby('SK_ID_CURR').AMT_CREDIT_SUM_DEBT.agg(['sum']).reset_index()\n",
    "bureau_sum_debt_closed = bureau_sum_debt[bureau_sum_debt['CREDIT_ACTIVE'] == 'Closed'].\\\n",
    "    groupby('SK_ID_CURR').AMT_CREDIT_SUM_DEBT.agg(['sum']).reset_index()\n",
    "bureau_credit_overdue = bureau_overdue.groupby('SK_ID_CURR').CREDIT_DAY_OVERDUE.agg([\n",
    "        'min', \n",
    "        'max', \n",
    "        'mean', \n",
    "        'std']).reset_index()\n",
    "bureau_credit_overdue.fillna(0,inplace=True)\n",
    "bureau_credit_prolong = pd.DataFrame(bureau[['SK_ID_CURR', 'CNT_CREDIT_PROLONG']])\\\n",
    "    .groupby('SK_ID_CURR').CNT_CREDIT_PROLONG.agg([\n",
    "        'min', \n",
    "        'max', \n",
    "        'mean', \n",
    "        'std']).reset_index()\n",
    "bureau_currency_count = pd.DataFrame(bureau[['SK_ID_CURR', 'CREDIT_CURRENCY']])\\\n",
    "    .groupby('SK_ID_CURR').CREDIT_CURRENCY.agg(['count']).reset_index()\n",
    "bureau_sum_overdue = pd.DataFrame(bureau[['SK_ID_CURR', 'AMT_CREDIT_SUM_OVERDUE']])\\\n",
    "    .groupby('SK_ID_CURR').AMT_CREDIT_SUM_OVERDUE.agg(['sum']).reset_index()\n",
    "bureau_sum_overdue.fillna(0,inplace=True)\n",
    "bureau_max_overdue = pd.DataFrame(bureau[['SK_ID_CURR', 'AMT_CREDIT_MAX_OVERDUE']])\\\n",
    "    .groupby('SK_ID_CURR').AMT_CREDIT_MAX_OVERDUE.agg(['min','max']).reset_index()\n",
    "bureau_max_overdue.fillna(0,inplace=True)\n",
    "bureau_credit_active_enddate = bureau[bureau['CREDIT_ACTIVE'] == 'Active'].\\\n",
    "    groupby('SK_ID_CURR').DAYS_CREDIT_ENDDATE.agg(['max','min','mean']).reset_index()\n",
    "bureau_credit_closed_enddate = bureau[bureau['CREDIT_ACTIVE'] == 'Closed'].\\\n",
    "    groupby('SK_ID_CURR').DAYS_CREDIT_ENDDATE.agg(['max','min','mean']).reset_index()\n",
    "\n",
    "    \n",
    "train=train.merge(bureaue_active_credit_limit,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureaue_active_credit_limit,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'sum': 'bureaue_active_credit_limit'}, inplace=True)\n",
    "test.rename(columns={'sum': 'bureaue_active_credit_limit'}, inplace=True)\n",
    "\n",
    "train=train.merge(bureau_credit_active_enddate,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_credit_active_enddate,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'max': 'bureau_credit_active_enddate_max','min': 'bureau_credit_active_enddate_min',\n",
    "                     'mean': 'bureau_credit_active_enddate_mean',}, inplace=True)\n",
    "test.rename(columns={'max': 'bureau_credit_active_enddate_max','min': 'bureau_credit_active_enddate_min',\n",
    "                     'mean': 'bureau_credit_active_enddate_mean',}, inplace=True)\n",
    "train=train.merge(bureau_credit_closed_enddate,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_credit_closed_enddate,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'max': 'bureau_credit_closed_enddate_max','min': 'bureau_credit_closed_enddate_min',\n",
    "                     'mean': 'bureau_credit_closed_enddate_mean',}, inplace=True)\n",
    "test.rename(columns={'max': 'bureau_credit_closed_enddate_max','min': 'bureau_credit_closed_enddate_min',\n",
    "                     'mean': 'bureau_credit_closed_enddate_mean',}, inplace=True)\n",
    "train['bureau_credit_active_enddate_max'].fillna(0,inplace=True)\n",
    "train['bureau_credit_active_enddate_min'].fillna(0,inplace=True)\n",
    "train['bureau_credit_active_enddate_mean'].fillna(0,inplace=True)\n",
    "train['bureau_credit_closed_enddate_max'].fillna(0,inplace=True)\n",
    "train['bureau_credit_closed_enddate_min'].fillna(0,inplace=True)\n",
    "train['bureau_credit_closed_enddate_mean'].fillna(0,inplace=True)\n",
    "test['bureau_credit_active_enddate_max'].fillna(0,inplace=True)\n",
    "test['bureau_credit_active_enddate_min'].fillna(0,inplace=True)\n",
    "test['bureau_credit_active_enddate_mean'].fillna(0,inplace=True)\n",
    "test['bureau_credit_closed_enddate_max'].fillna(0,inplace=True)\n",
    "test['bureau_credit_closed_enddate_min'].fillna(0,inplace=True)\n",
    "test['bureau_credit_closed_enddate_mean'].fillna(0,inplace=True)\n",
    "\n",
    "\n",
    "train=train.merge(bureau_credit_active,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_credit_active,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'count': 'bureau_credit_active_count'}, inplace=True)\n",
    "test.rename(columns={'count': 'bureau_credit_active_count'}, inplace=True)\n",
    "\n",
    "train=train.merge(bureau_credit_closed,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_credit_closed,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'count': 'bureau_credit_closed_count'}, inplace=True)\n",
    "test.rename(columns={'count': 'bureau_credit_closed_count'}, inplace=True)\n",
    "\n",
    "train=train.merge(bureau_credit_type_active,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_credit_type_active,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'count': 'bureau_credit_type_active_count'}, inplace=True)\n",
    "test.rename(columns={'count': 'bureau_credit_type_active_count'}, inplace=True)\n",
    "\n",
    "train=train.merge(bureau_credit_type_closed,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_credit_type_closed,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'count': 'bureau_credit_type_closed_count'}, inplace=True)\n",
    "test.rename(columns={'count': 'bureau_credit_type_closed_count'}, inplace=True)\n",
    "\n",
    "train=train.merge(bureau_sum_debt_active,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_sum_debt_active,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'sum': 'bureau_sum_debt_active_sum'}, inplace=True)\n",
    "test.rename(columns={'sum': 'bureau_sum_debt_active_sum'}, inplace=True)\n",
    "\n",
    "train=train.merge(bureau_sum_debt_closed,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_sum_debt_closed,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'sum': 'bureau_sum_debt_closed_sum'}, inplace=True)\n",
    "test.rename(columns={'sum': 'bureau_sum_debt_closed_sum'}, inplace=True)\n",
    "\n",
    "train=train.merge(bureau_currency_count,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_currency_count,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'count': 'bureau_currency_count'}, inplace=True)\n",
    "test.rename(columns={'count': 'bureau_currency_count'}, inplace=True)\n",
    "\n",
    "train=train.merge(bureau_sum_overdue,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_sum_overdue,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'sum': 'bureau_sum_overdue'}, inplace=True)\n",
    "test.rename(columns={'sum': 'bureau_sum_overdue'}, inplace=True)\n",
    "\n",
    "train=train.merge(bureau_max_overdue,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_max_overdue,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'max': 'bureau_max_overdue_max','min': 'bureau_max_overdue_min'}, inplace=True)\n",
    "test.rename(columns={'max': 'bureau_max_overdue_max','min': 'bureau_max_overdue_min'}, inplace=True)\n",
    "\n",
    "train=train.merge(bureau_credit_prolong,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_credit_prolong,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'min': 'bureau_credit_prolong_min','max': 'bureau_credit_prolong_max',\n",
    "                      'mean': 'bureau_credit_prolong_mean','std': 'bureau_credit_prolong_std'}, inplace=True)\n",
    "test.rename(columns={'min': 'bureau_credit_prolong_min','max': 'bureau_credit_prolong_max',\n",
    "                      'mean': 'bureau_credit_prolong_mean','std': 'bureau_credit_prolong_std'}, inplace=True)\n",
    "\n",
    "train=train.merge(bureau_credit_overdue,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_credit_overdue,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'min': 'bureau_credit_overdue_min','max': 'bureau_credit_overdue_max',\n",
    "                      'mean': 'bureau_credit_overdue_mean','std': 'bureau_credit_overdue_std'}, inplace=True)\n",
    "test.rename(columns={'min': 'bureau_credit_overdue_min','max': 'bureau_credit_overdue_max',\n",
    "                      'mean': 'bureau_credit_overdue_mean','std': 'bureau_credit_overdue_std'}, inplace=True)\n",
    "train['bureau_credit_overdue_min'].fillna(0,inplace=True)\n",
    "train['bureau_credit_overdue_max'].fillna(0,inplace=True)\n",
    "train['bureau_credit_overdue_mean'].fillna(0,inplace=True)\n",
    "train['bureau_credit_overdue_std'].fillna(0,inplace=True)\n",
    "train['bureau_credit_prolong_min'].fillna(0,inplace=True)\n",
    "train['bureau_credit_prolong_max'].fillna(0,inplace=True)\n",
    "train['bureau_credit_prolong_mean'].fillna(0,inplace=True)\n",
    "train['bureau_credit_prolong_std'].fillna(0,inplace=True)\n",
    "train['bureau_credit_active_count'].fillna(0,inplace=True)\n",
    "train['bureau_credit_closed_count'].fillna(0,inplace=True)\n",
    "train['bureau_sum_debt_active_sum'].fillna(0,inplace=True)\n",
    "train['bureau_sum_debt_closed_sum'].fillna(0,inplace=True)\n",
    "train['bureau_credit_type_active_count'].fillna(0,inplace=True)\n",
    "train['bureau_credit_type_closed_count'].fillna(0,inplace=True)\n",
    "train['bureau_currency_count'].fillna(0,inplace=True)\n",
    "train['bureau_sum_overdue'].fillna(0,inplace=True)\n",
    "train['bureau_max_overdue_max'].fillna(0,inplace=True)\n",
    "train['bureau_max_overdue_min'].fillna(0,inplace=True)\n",
    "train['bureaue_active_credit_limit'].fillna(0,inplace=True)\n",
    "test['bureau_credit_overdue_min'].fillna(0,inplace=True)\n",
    "test['bureau_credit_overdue_max'].fillna(0,inplace=True)\n",
    "test['bureau_credit_overdue_mean'].fillna(0,inplace=True)\n",
    "test['bureau_credit_overdue_std'].fillna(0,inplace=True)\n",
    "test['bureau_credit_prolong_min'].fillna(0,inplace=True)\n",
    "test['bureau_credit_prolong_max'].fillna(0,inplace=True)\n",
    "test['bureau_credit_prolong_mean'].fillna(0,inplace=True)\n",
    "test['bureau_credit_prolong_std'].fillna(0,inplace=True)\n",
    "test['bureau_credit_active_count'].fillna(0,inplace=True)\n",
    "test['bureau_credit_closed_count'].fillna(0,inplace=True)\n",
    "test['bureau_sum_debt_active_sum'].fillna(0,inplace=True)\n",
    "test['bureau_sum_debt_closed_sum'].fillna(0,inplace=True)\n",
    "test['bureau_credit_type_active_count'].fillna(0,inplace=True)\n",
    "test['bureau_credit_type_closed_count'].fillna(0,inplace=True)\n",
    "test['bureau_currency_count'].fillna(0,inplace=True)\n",
    "test['bureau_sum_overdue'].fillna(0,inplace=True)\n",
    "test['bureau_max_overdue_max'].fillna(0,inplace=True)\n",
    "test['bureau_max_overdue_min'].fillna(0,inplace=True)\n",
    "test['bureaue_active_credit_limit'].fillna(0,inplace=True)\n",
    "\n",
    "train.head()\n",
    "                            "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bureau_credit_prolong['mean'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train['TARGET'].value_counts()\n",
    "\n",
    "# it is an unbalanced data, 8.5% of the target is 1, so the baseline is around 92%"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train['NAME_FAMILY_STATUS'].value_counts()\n",
    "train['FAMILY_STATUS_SINGLE'] = train['NAME_FAMILY_STATUS']\\\n",
    "    .apply(lambda x: 1 if x in ['Single / not married','Separated','Widow'] else 0)\n",
    "test['FAMILY_STATUS_SINGLE'] = test['NAME_FAMILY_STATUS']\\\n",
    "    .apply(lambda x: 1 if x in ['Single / not married','Separated','Widow'] else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'NAME_CONTRACT_TYPE', 2 values, converting to 0/1\n",
    "train['NAME_CONTRACT_TYPE'] = train['NAME_CONTRACT_TYPE'].apply(lambda x: 0 if x == 'Cash loans' else 1)\n",
    "test['NAME_CONTRACT_TYPE'] = test['NAME_CONTRACT_TYPE'].apply(lambda x: 0 if x == 'Cash loans' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CODE_GENDER', drop XNA as only 4 rows, convert the rest to 0/1\n",
    "train = train[train['CODE_GENDER'] != 'XNA']\n",
    "train['CODE_GENDER'] = train['CODE_GENDER'].apply(lambda x: 0 if x == 'F' else 1)\n",
    "test['CODE_GENDER'] = test['CODE_GENDER'].apply(lambda x: 0 if x == 'F' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAG_OWN_CAR\n",
    "train['FLAG_OWN_CAR'] = train['FLAG_OWN_CAR'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "test['FLAG_OWN_CAR'] = test['FLAG_OWN_CAR'].apply(lambda x: 1 if x == 'Y' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAG_OWN_REALTY\n",
    "train['FLAG_OWN_REALTY'] = train['FLAG_OWN_REALTY'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "test['FLAG_OWN_REALTY'] = test['FLAG_OWN_REALTY'].apply(lambda x: 1 if x == 'Y' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where null it should be dropped or mean or average income/annuity\n",
    "\n",
    "avgAnnuityRate = (train['AMT_ANNUITY']/train['AMT_CREDIT']).mean()\n",
    "train['AMT_ANNUITY'].fillna(avgAnnuityRate * train['AMT_CREDIT'],inplace=True)\n",
    "test['AMT_ANNUITY'].fillna(avgAnnuityRate * train['AMT_CREDIT'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  where null mean or average income / goods price\n",
    "goodsPriceMean = train['AMT_GOODS_PRICE'].mean()\n",
    "train['AMT_GOODS_PRICE'].fillna(goodsPriceMean,inplace=True)\n",
    "test['AMT_GOODS_PRICE'].fillna(goodsPriceMean,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  categorical, dummify, where null either unknown or most frequent\n",
    "train['NAME_TYPE_SUITE'].fillna('Unaccompanied',inplace=True)\n",
    "test['NAME_TYPE_SUITE'].fillna('Unaccompanied',inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# convert days to years\n",
    "train['DAYS_BIRTH'] = train['DAYS_BIRTH'] / -365\n",
    "test['DAYS_BIRTH'] = test['DAYS_BIRTH'] / -365"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# numeric\n",
    "train['REGION_POPULATION_RELATIVE'].hist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: negative, numeric\n",
    "train['DAYS_BIRTH'].hist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: negative, numeric\n",
    "train['DAYS_EMPLOYED'].head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: negative, numeric\n",
    "train['DAYS_REGISTRATION'].head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: negative, numeric\n",
    "train['DAYS_ID_PUBLISH'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric, the older the worse, where null check own car\n",
    "train['OWN_CAR_AGE'].fillna(100,inplace=True)\n",
    "test['OWN_CAR_AGE'].fillna(100,inplace=True)\n",
    "train['OWN_CAR_AGE'] = train['OWN_CAR_AGE'] * -1\n",
    "test['OWN_CAR_AGE'] = test['OWN_CAR_AGE'] * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric, drop where it is null\n",
    "train = train[train['CNT_FAM_MEMBERS'] > 0]\n",
    "test = test[test['CNT_FAM_MEMBERS'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.columns.tolist()\n",
    "train['INCOME_ANNUITY_RATIO'] = train['AMT_ANNUITY'] / train['AMT_INCOME_TOTAL']\n",
    "test['INCOME_ANNUITY_RATIO'] = test['AMT_ANNUITY'] / test['AMT_INCOME_TOTAL']\n",
    "\n",
    "train['INCOME_CREDIT_RATIO'] = train['AMT_CREDIT'] / train['AMT_INCOME_TOTAL']\n",
    "test['INCOME_CREDIT_RATIO'] = test['AMT_CREDIT'] / test['AMT_INCOME_TOTAL']\n",
    "\n",
    "train['ANNUITY_CREDIT_RATIO'] = train['AMT_CREDIT'] / train['AMT_ANNUITY']\n",
    "test['ANNUITY_CREDIT_RATIO'] = test['AMT_CREDIT'] / test['AMT_ANNUITY']\n",
    "\n",
    "train['ANNUITY_GOODS_PRICE_RATIO'] = train['AMT_ANNUITY'] / train['AMT_GOODS_PRICE']\n",
    "test['ANNUITY_GOODS_PRICE_RATIO'] = test['AMT_ANNUITY'] / test['AMT_GOODS_PRICE']\n",
    "\n",
    "train['ANNUITY_DAYS_EMPLOYED_RATIO'] = train['AMT_ANNUITY'] / train['DAYS_EMPLOYED']\n",
    "test['ANNUITY_DAYS_EMPLOYED_RATIO'] = test['AMT_ANNUITY'] / test['DAYS_EMPLOYED']\n",
    "\n",
    "train['CREDIT_DAYS_EMPLOYED_RATIO'] = train['AMT_CREDIT'] / train['DAYS_EMPLOYED']\n",
    "test['CREDIT_DAYS_EMPLOYED_RATIO'] = test['AMT_CREDIT'] / test['DAYS_EMPLOYED']\n",
    "\n",
    "train['GOODS_CREDIT_RATIO'] = train['AMT_CREDIT'] / train['AMT_GOODS_PRICE']\n",
    "test['GOODS_CREDIT_RATIO'] = test['AMT_CREDIT'] / test['AMT_GOODS_PRICE']\n",
    "\n",
    "train['INCOME_PER_CAPITA'] = train['AMT_INCOME_TOTAL'] / train['CNT_FAM_MEMBERS']\n",
    "test['INCOME_PER_CAPITA'] = test['AMT_INCOME_TOTAL'] / test['CNT_FAM_MEMBERS']\n",
    "\n",
    "train['EXT_SOURCES_MEAN'] = train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "test['EXT_SOURCES_MEAN'] = test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1)\n",
    "\n",
    "train['EXT_SOURCES_PROD'] = train['EXT_SOURCE_1'] * train['EXT_SOURCE_2'] * train['EXT_SOURCE_3']\n",
    "test['EXT_SOURCES_PROD'] = test['EXT_SOURCE_1'] * test['EXT_SOURCE_2'] * test['EXT_SOURCE_3']\n",
    "\n",
    "train['EXT_SCORES_STD'] = train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "train['EXT_SCORES_STD'] = train['EXT_SCORES_STD'].fillna(train['EXT_SCORES_STD'].mean())\n",
    "test['EXT_SCORES_STD'] = test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "test['EXT_SCORES_STD'] = test['EXT_SCORES_STD'].fillna(train['EXT_SCORES_STD'].mean())\n",
    "\n",
    "train['EMPLOY_TO_BIRTH_RATIO'] = train['DAYS_EMPLOYED'] / train['DAYS_BIRTH']\n",
    "test['EMPLOY_TO_BIRTH_RATIO'] = test['DAYS_EMPLOYED'] / test['DAYS_BIRTH']\n",
    "\n",
    "train['PHONE_TO_EMPLOY_RATIO'] = train['DAYS_LAST_PHONE_CHANGE'] / train['DAYS_EMPLOYED']\n",
    "test['PHONE_TO_EMPLOY_RATIO'] = test['DAYS_LAST_PHONE_CHANGE'] / test['DAYS_EMPLOYED']\n",
    "\n",
    "train['PHONE_TO_BIRTH_RATIO'] = train['DAYS_LAST_PHONE_CHANGE'] / train['DAYS_BIRTH']\n",
    "test['PHONE_TO_BIRTH_RATIO'] = test['DAYS_LAST_PHONE_CHANGE'] / test['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null with mean for _1, _2, _3\n",
    "'''mean1 = train['EXT_SOURCE_1'].mean()\n",
    "mean2 = train['EXT_SOURCE_2'].mean()\n",
    "mean3 = train['EXT_SOURCE_3'].mean()\n",
    "train['EXT_SOURCE_1'].fillna(mean1,inplace=True)\n",
    "train['EXT_SOURCE_2'].fillna(mean2,inplace=True)\n",
    "train['EXT_SOURCE_3'].fillna(mean3,inplace=True)\n",
    "test['EXT_SOURCE_1'].fillna(mean1,inplace=True)\n",
    "test['EXT_SOURCE_2'].fillna(mean2,inplace=True)\n",
    "test['EXT_SOURCE_3'].fillna(mean3,inplace=True)'''\n",
    "train['EXT_SOURCE_1'].fillna(0,inplace=True)\n",
    "train['EXT_SOURCE_2'].fillna(0,inplace=True)\n",
    "train['EXT_SOURCE_3'].fillna(0,inplace=True)\n",
    "test['EXT_SOURCE_1'].fillna(0,inplace=True)\n",
    "test['EXT_SOURCE_2'].fillna(0,inplace=True)\n",
    "test['EXT_SOURCE_3'].fillna(0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with mean all _AVG, _MEDI, _MODE\n",
    "for col in train.columns.tolist():\n",
    "    if (col.endswith('_AVG') or col.endswith('_MEDI') or col.endswith('_MODE')) and col not in ['FONDKAPREMONT_MODE','HOUSETYPE_MODE',\n",
    "                    'WALLSMATERIAL_MODE','EMERGENCYSTATE_MODE']: \n",
    "        #print (col)\n",
    "        mean = train[col].mean()\n",
    "        train[col].fillna(mean,inplace=True)\n",
    "        test[col].fillna(mean,inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMERGENCYSTATE_MODE\n",
    "train['EMERGENCYSTATE_MODE'] = train['EMERGENCYSTATE_MODE'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "test['EMERGENCYSTATE_MODE'] = test['EMERGENCYSTATE_MODE'].apply(lambda x: 1 if x == 'Y' else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fill none with mean or median for all circle\n",
    "for col in train.columns.tolist():\n",
    "    if col.endswith('_CIRCLE'):\n",
    "        mean = train[col].mean()\n",
    "        train[col].fillna(mean,inplace=True)\n",
    "        test[col].fillna(mean,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative numeric, drop where it is null\n",
    "train['DAYS_LAST_PHONE_CHANGE'].fillna(0,inplace=True)\n",
    "test['DAYS_LAST_PHONE_CHANGE'].fillna(0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all amt_credit req, maybe bin it\n",
    "train['AMT_REQ_CREDIT_BUREAU_YEAR'].mean()\n",
    "\n",
    "for col in train.columns.tolist():\n",
    "    if 'AMT_REQ_CREDIT_BUREAU_' in col:\n",
    "        #print (col)\n",
    "        mean = train[col].mean()\n",
    "        train[col].fillna(mean,inplace=True)\n",
    "        test[col].fillna(mean,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objs_num = len(train)\n",
    "dataset = pd.concat(objs=[train, test], axis=0)\n",
    "dataset = pd.get_dummies(dataset, \n",
    "    columns = ['NAME_TYPE_SUITE','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_HOUSING_TYPE',\n",
    "            'OCCUPATION_TYPE','ORGANIZATION_TYPE','FONDKAPREMONT_MODE',\n",
    "            'HOUSETYPE_MODE','WALLSMATERIAL_MODE'],prefix_sep='__') # ,'WEEKDAY_APPR_PROCESS_START'\n",
    "train = dataset[:train_objs_num]\n",
    "test = dataset[train_objs_num:]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "correlations = train.corr()['TARGET'].sort_values()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train['FLAG_CONT_MOBILE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "low_correlation_cols = ['ORGANIZATION_TYPE__Trade: type 5','ORGANIZATION_TYPE__Transport: type 2',\n",
    "                        'NONLIVINGAPARTMENTS_MODE','FLAG_DOCUMENT_12','ORGANIZATION_TYPE__Telecom',\n",
    "                        'ORGANIZATION_TYPE__Industry: type 6','bureau_credit_prolong_min','ORGANIZATION_TYPE__Housing',\n",
    "                        'OCCUPATION_TYPE__Realty agents','NAME_HOUSING_TYPE__Co-op apartment','FLAG_DOCUMENT_5',\n",
    "                        'ORGANIZATION_TYPE__Legal Services','ORGANIZATION_TYPE__Industry: type 7',\n",
    "                        'ORGANIZATION_TYPE__Advertising','FLAG_DOCUMENT_20',\n",
    "                        'ORGANIZATION_TYPE__Business Entity Type 1','FLAG_CONT_MOBILE',\n",
    "                        'NAME_TYPE_SUITE__Group of people','FLAG_MOBIL','WALLSMATERIAL_MODE__Others',\n",
    "                        'AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_HOUR','HOUSETYPE_MODE__terraced house',\n",
    "                       'WEEKDAY_APPR_PROCESS_START']\n",
    "\n",
    "X = train.drop(['SK_ID_CURR','TARGET'] + low_correlation_cols, axis=1)\n",
    "y = train['TARGET']\n",
    "X_test = test.drop(['SK_ID_CURR','TARGET'] + low_correlation_cols, axis=1)\n",
    "\n",
    "#X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainx = train.drop(['SK_ID_CURR'] + low_correlation_cols, axis=1)\n",
    "correlations = trainx.corr()['TARGET'].sort_values()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "threshold = 0.9\n",
    "\n",
    "# Absolute value correlation matrix\n",
    "corr_matrix = train.corr().abs()\n",
    "corr_matrix.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "print('There are %d columns to remove: %s.' % (len(to_drop), str(to_drop)))\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "X = X.drop(columns = to_drop, errors='ignore')\n",
    "X_test = X_test.drop(columns = to_drop, errors='ignore')\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "\n",
    "import gc\n",
    "import csv\n",
    "\n",
    "cnt = 0\n",
    "p_buf = []\n",
    "n_splits = 2\n",
    "n_repeats = 2\n",
    "kf = RepeatedKFold(\n",
    "    n_splits=n_splits, \n",
    "    n_repeats=n_repeats, \n",
    "    random_state=0)\n",
    "auc_buf = []   \n",
    "\n",
    "params = {\n",
    "            'nthread':4,\n",
    "            'n_estimators':10000,\n",
    "            'learning_rate':0.02,\n",
    "            'num_leaves':34,\n",
    "            'colsample_bytree':0.9497036,\n",
    "            'subsample':0.8715623,\n",
    "            'subsample_freq':1,\n",
    "            'max_depth':8,\n",
    "            'reg_alpha':0.041545473,\n",
    "            'reg_lambda':0.0735294,\n",
    "            'min_split_gain':0.0222415,\n",
    "            'min_child_weight':39.3259775,\n",
    "            'random_state':0,\n",
    "    'feature_fraction': 0.15,\n",
    "            'verbose':-1,\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "}\n",
    "\n",
    "'''params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'max_depth': 12,\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.025,\n",
    "        'feature_fraction': 0.85,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0,\n",
    "        'num_threads': 8,\n",
    "        'lambda_l2': 1.5,\n",
    "        'min_gain_to_split': 0,\n",
    "    }  \n",
    "'''\n",
    "for train_index, valid_index in kf.split(X):\n",
    "    print('Fold {}/{}'.format(cnt + 1, n_splits))\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb.Dataset(X.loc[train_index], y.loc[train_index], feature_name=X.columns.tolist()),\n",
    "        num_boost_round=10000,\n",
    "        valid_sets=[lgb.Dataset(X.loc[valid_index], y.loc[valid_index])],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=100,\n",
    "    )\n",
    "\n",
    "    if cnt == 0:\n",
    "        importance = model.feature_importance()\n",
    "        model_fnames = model.feature_name()\n",
    "        tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]\n",
    "        tuples = [x for x in tuples if x[1] > 0]\n",
    "        print('Important features:')\n",
    "        print(tuples[:200])\n",
    "\n",
    "    p = model.predict(X.loc[valid_index], num_iteration=model.best_iteration)\n",
    "    #auc = roc_auc_score(y.loc[valid_index], p)\n",
    "\n",
    "    #print('{} AUC: {}'.format(cnt, auc))\n",
    "\n",
    "    p = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    if len(p_buf) == 0:\n",
    "        p_buf = np.array(p)\n",
    "    else:\n",
    "        p_buf += np.array(p)\n",
    "    #auc_buf.append(auc)\n",
    "\n",
    "    cnt += 1\n",
    "    if cnt > 0: # Comment this to run several folds\n",
    "        break\n",
    "    \n",
    "    del model\n",
    "    gc.collect\n",
    "\n",
    "#auc_mean = np.mean(auc_buf)\n",
    "#auc_std = np.std(auc_buf)\n",
    "#print('AUC = {:.6f} +/- {:.6f}'.format(auc_mean, auc_std))\n",
    "\n",
    "preds = p_buf/cnt\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['SK_ID_CURR'] = test['SK_ID_CURR']\n",
    "subm['TARGET'] = preds\n",
    "subm.to_csv('home-default-risk_lgbm.csv', index=False,quoting=csv.QUOTE_NONNUMERIC)\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"e:/xgboost/python-package\")\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "#model = XGBClassifier()\n",
    "#model.fit(x_train, y_train)\n",
    "#print (model.score(x_train, y_train))\n",
    "\n",
    "logit_model = XGBClassifier() #LogisticRegression()  \n",
    "# Fit\n",
    "#logit_model = logit_model.fit(X_train, y_train,eval_metric='roc_auc')  \n",
    "# How accurate?\n",
    "#print (logit_model.score(X_train, y_train))  \n",
    "#0.7874\n",
    "\n",
    "# How does it perform on the test dataset?\n",
    "# Fit\n",
    "logit_model = logit_model.fit(X, y,eval_metric='auc')  \n",
    "# Predictions on the test dataset\n",
    "predicted = pd.DataFrame(logit_model.predict(X_test))  \n",
    "# Probabilities on the test dataset\n",
    "probs = pd.DataFrame(logit_model.predict_proba(X_test))  \n",
    "#prd[:,i] = np.round(probs,5)[:,1]\n",
    "#print (metrics.accuracy_score(y_test, predicted)  )\n",
    "\n",
    "print (probs.shape)\n",
    "\n",
    "prd_1 = pd.DataFrame(probs)\n",
    "\n",
    "import csv\n",
    "\n",
    "submit = pd.concat([test['SK_ID_CURR'],prd_1],axis=1)\n",
    "\n",
    "print (submit.columns.tolist)\n",
    "\n",
    "submit = submit.drop(submit.columns[1], axis=1)\n",
    "#probs.head()\n",
    "submit.to_csv('home-default-risk_xgb.csv',index=False,quoting=csv.QUOTE_NONNUMERIC)\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_best_model_and_accuracy(model, params, X, y):\n",
    "    grid = GridSearchCV(model, # the model to grid search\n",
    "                        params, # the parameter set to try \n",
    "                        error_score=0., scoring='roc_auc') # if a parameter set raises an error, continue and set the performance as a big, fat 0\n",
    "    grid.fit(X, y) # fit the model and parameters\n",
    "    # our classical metric for performance\n",
    "    print (\"Best Accuracy: {}\".format(grid.best_score_))\n",
    "    # the best parameters that caused the best accuracy\n",
    "    print (\"Best Parameters: {}\".format(grid.best_params_))\n",
    "    # the average time it took a model to fit to the data (in seconds)\n",
    "    print (\"Average Time to Fit (s): {}\".format(round(grid.cv_results_['mean_fit_time'].mean(), 3)))\n",
    "    # the average time it took a model to predict out of sample data (in seconds)\n",
    "    # this metric gives us insight into how this model will perform in real-time analysis\n",
    "    print (\"Average Time to Score (s): {}\".format(round(grid.cv_results_['mean_score_time'].mean(), 3)))\n",
    "\n",
    "# Logistic Regression\n",
    "lr_params = {'C':[1e-1, 1e0, 1e1, 1e2], 'penalty':['l1', 'l2']}\n",
    "\n",
    "# KNN\n",
    "knn_params = {'n_neighbors': [1, 3, 5, 7]}\n",
    "\n",
    "# Decision Tree\n",
    "tree_params = {'max_depth':[None, 1, 3, 5, 7]}\n",
    "\n",
    "# Random Forest\n",
    "forest_params = {'n_estimators': [10, 50, 100], 'max_depth': [None, 1, 3, 5, 7]}\n",
    "\n",
    "lr = LogisticRegression(penalty='l1',n_jobs=-1)\n",
    "knn = KNeighborsClassifier()\n",
    "d_tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "#get_best_model_and_accuracy(lr, lr_params, X[:3000], y[:3000])\n",
    "#get_best_model_and_accuracy(knn, knn_params, X[:3000], y[:3000])\n",
    "#get_best_model_and_accuracy(d_tree, tree_params, X[:3000], y[:3000])\n",
    "#get_best_model_and_accuracy(forest, forest_params, X[:3000], y[:3000])\n",
    "print ('Fitting...')\n",
    "\n",
    "lr.fit(X,y)\n",
    "probs = lr.predict_proba(X_test)\n",
    "print ('Predicting...')\n",
    "\n",
    "print (lr.score(X,y))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "prd_1 = pd.DataFrame(probs)\n",
    "\n",
    "submit = pd.concat([test['SK_ID_CURR'],prd_1],axis=1)\n",
    "\n",
    "print (submit.columns.tolist)\n",
    "\n",
    "submit = submit.drop(submit.columns[1], axis=1)\n",
    "#probs.head()\n",
    "submit.to_csv('home-default-risk.csv',index=False,quoting=csv.QUOTE_NONNUMERIC)\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ax1 = sns.distplot(train[\"AMT_CREDIT\"][train.TARGET==1], color='y')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "sns.kdeplot(train.loc[train['TARGET'] == 0, 'DAYS_BIRTH'], label = 'Repaid Loan')\n",
    "sns.kdeplot(train.loc[train['TARGET'] == 1, 'DAYS_BIRTH'], label = 'Not Repaid Loan')\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Ages');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
