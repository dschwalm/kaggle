{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import re\n",
    "import gc\n",
    "import sys, os, random\n",
    "\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(37)\n",
    "random.seed(17)\n",
    "\n",
    "pd.set_option('display.max_rows',1000)\n",
    "\n",
    "#root = '/Users/schwalmdaniel/github/kaggle/home-credit-default-risk'\n",
    "root = 'e:/kaggle/home-credit-default-risk'\n",
    "\n",
    "train=pd.read_csv(root + \"/application_train.csv\")\n",
    "test=pd.read_csv(root + \"/application_test.csv\")\n",
    "bureau=pd.read_csv(root + \"/bureau.csv\")\n",
    "'''bureau_balance=pd.read_csv(root + \"/bureau_balance.csv\")\n",
    "POS_CASH_balance=pd.read_csv(root + \"/POS_CASH_balance.csv\")\n",
    "credit_card_balance=pd.read_csv(root + \"/credit_card_balance.csv\")\n",
    "previous_application=pd.read_csv(root + \"/previous_application.csv\")\n",
    "installments_payments=pd.read_csv(root + \"/installments_payments.csv\")'''\n",
    "\n",
    "# have a look at the ds\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_credit = pd.DataFrame(bureau[['SK_ID_CURR', 'CREDIT_ACTIVE']])\n",
    "bureau_overdue = pd.DataFrame(bureau[['SK_ID_CURR', 'CREDIT_DAY_OVERDUE']])\n",
    "bureau_currency = pd.DataFrame(bureau[['SK_ID_CURR', 'CREDIT_CURRENCY']])\n",
    "\n",
    "bureau_credit_active = bureau_credit[bureau_credit['CREDIT_ACTIVE'] == 'Active'].\\\n",
    "    groupby('SK_ID_CURR').CREDIT_ACTIVE.agg(['count']).reset_index()\n",
    "bureau_credit_closed = bureau_credit[bureau_credit['CREDIT_ACTIVE'] == 'Closed'].\\\n",
    "    groupby('SK_ID_CURR').CREDIT_ACTIVE.agg(['count']).reset_index()\n",
    "bureau_credit_overdue = bureau_overdue.groupby('SK_ID_CURR').CREDIT_DAY_OVERDUE.agg([\n",
    "        'min', \n",
    "        'max', \n",
    "        'mean', \n",
    "        'std']).reset_index()\n",
    "bureau_credit_overdue.fillna(0,inplace=True)\n",
    "bureau_currency_count = bureau_currency.groupby('SK_ID_CURR').CREDIT_CURRENCY.agg(['count']).reset_index()\n",
    "\n",
    "train=train.merge(bureau_credit_active,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_credit_active,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'count': 'bureau_credit_active_count'}, inplace=True)\n",
    "test.rename(columns={'count': 'bureau_credit_active_count'}, inplace=True)\n",
    "\n",
    "train=train.merge(bureau_currency_count,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_currency_count,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'count': 'bureau_currency_count'}, inplace=True)\n",
    "test.rename(columns={'count': 'bureau_currency_count'}, inplace=True)\n",
    "\n",
    "train=train.merge(bureau_credit_closed,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_credit_closed,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'count': 'bureau_credit_closed_count'}, inplace=True)\n",
    "test.rename(columns={'count': 'bureau_credit_closed_count'}, inplace=True)\n",
    "\n",
    "train=train.merge(bureau_credit_overdue,on='SK_ID_CURR', how='left')\n",
    "test=test.merge(bureau_credit_overdue,on='SK_ID_CURR', how='left')\n",
    "train.rename(columns={'min': 'bureau_credit_overdue_min','max': 'bureau_credit_overdue_max',\n",
    "                      'mean': 'bureau_credit_overdue_mean','std': 'bureau_credit_overdue_std'}, inplace=True)\n",
    "test.rename(columns={'min': 'bureau_credit_overdue_min','max': 'bureau_credit_overdue_max',\n",
    "                      'mean': 'bureau_credit_overdue_mean','std': 'bureau_credit_overdue_std'}, inplace=True)\n",
    "train['bureau_credit_overdue_min'].fillna(0,inplace=True)\n",
    "train['bureau_credit_overdue_max'].fillna(0,inplace=True)\n",
    "train['bureau_credit_overdue_mean'].fillna(0,inplace=True)\n",
    "train['bureau_credit_overdue_std'].fillna(0,inplace=True)\n",
    "train['bureau_credit_active_count'].fillna(0,inplace=True)\n",
    "train['bureau_credit_closed_count'].fillna(0,inplace=True)\n",
    "train['bureau_currency_count'].fillna(0,inplace=True)\n",
    "test['bureau_credit_overdue_min'].fillna(0,inplace=True)\n",
    "test['bureau_credit_overdue_max'].fillna(0,inplace=True)\n",
    "test['bureau_credit_overdue_mean'].fillna(0,inplace=True)\n",
    "test['bureau_credit_overdue_std'].fillna(0,inplace=True)\n",
    "test['bureau_credit_active_count'].fillna(0,inplace=True)\n",
    "test['bureau_credit_closed_count'].fillna(0,inplace=True)\n",
    "test['bureau_currency_count'].fillna(0,inplace=True)\n",
    "\n",
    "train.head()\n",
    "                            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''                        .groupby('SK_ID_CURR').CREDIT_ACTIVE.agg(\\\n",
    "[\n",
    "    lambda x: len(np.unique(x)),\n",
    "])).reset_index()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TARGET'].value_counts()\n",
    "\n",
    "# it is an unbalanced data, 8.5% of the target is 1, so the baseline is around 92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'NAME_CONTRACT_TYPE', 2 values, converting to 0/1\n",
    "train['NAME_CONTRACT_TYPE'] = train['NAME_CONTRACT_TYPE'].apply(lambda x: 0 if x == 'Cash loans' else 1)\n",
    "test['NAME_CONTRACT_TYPE'] = test['NAME_CONTRACT_TYPE'].apply(lambda x: 0 if x == 'Cash loans' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CODE_GENDER', drop XNA as only 4 rows, convert the rest to 0/1\n",
    "train = train[train['CODE_GENDER'] != 'XNA']\n",
    "train['CODE_GENDER'] = train['CODE_GENDER'].apply(lambda x: 0 if x == 'F' else 1)\n",
    "test['CODE_GENDER'] = test['CODE_GENDER'].apply(lambda x: 0 if x == 'F' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAG_OWN_CAR\n",
    "train['FLAG_OWN_CAR'] = train['FLAG_OWN_CAR'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "test['FLAG_OWN_CAR'] = test['FLAG_OWN_CAR'].apply(lambda x: 1 if x == 'Y' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLAG_OWN_REALTY\n",
    "train['FLAG_OWN_REALTY'] = train['FLAG_OWN_REALTY'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "test['FLAG_OWN_REALTY'] = test['FLAG_OWN_REALTY'].apply(lambda x: 1 if x == 'Y' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where null it should be dropped or mean or average income/annuity\n",
    "\n",
    "avgAnnuityRate = (train['AMT_ANNUITY']/train['AMT_CREDIT']).mean()\n",
    "train['AMT_ANNUITY'].fillna(avgAnnuityRate * train['AMT_CREDIT'],inplace=True)\n",
    "test['AMT_ANNUITY'].fillna(avgAnnuityRate * train['AMT_CREDIT'],inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  where null mean or average income / goods price\n",
    "goodsPriceMean = train['AMT_GOODS_PRICE'].mean()\n",
    "train['AMT_GOODS_PRICE'].fillna(goodsPriceMean,inplace=True)\n",
    "test['AMT_GOODS_PRICE'].fillna(goodsPriceMean,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  categorical, dummify, where null either unknown or most frequent\n",
    "train['NAME_TYPE_SUITE'].fillna('Unaccompanied',inplace=True)\n",
    "test['NAME_TYPE_SUITE'].fillna('Unaccompanied',inplace=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# convert days to years\n",
    "train['DAYS_BIRTH'] = train['DAYS_BIRTH'] / -365\n",
    "test['DAYS_BIRTH'] = test['DAYS_BIRTH'] / -365"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# numeric\n",
    "train['REGION_POPULATION_RELATIVE'].hist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: negative, numeric\n",
    "train['DAYS_BIRTH'].hist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: negative, numeric\n",
    "train['DAYS_EMPLOYED'].head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: negative, numeric\n",
    "train['DAYS_REGISTRATION'].head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TODO: negative, numeric\n",
    "train['DAYS_ID_PUBLISH'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric, the older the worse, where null check own car\n",
    "train['OWN_CAR_AGE'].fillna(100,inplace=True)\n",
    "test['OWN_CAR_AGE'].fillna(100,inplace=True)\n",
    "train['OWN_CAR_AGE'] = train['OWN_CAR_AGE'] * -1\n",
    "test['OWN_CAR_AGE'] = test['OWN_CAR_AGE'] * -1\n",
    "train['OWN_CAR_AGE'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric, drop where it is null\n",
    "train = train[train['CNT_FAM_MEMBERS'] > 0]\n",
    "test = test[test['CNT_FAM_MEMBERS'] > 0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#train.columns.tolist()\n",
    "train['INCOME_ANNUITY_RATIO'] = train['AMT_ANNUITY'] / train['AMT_INCOME_TOTAL']\n",
    "test['INCOME_ANNUITY_RATIO'] = test['AMT_ANNUITY'] / test['AMT_INCOME_TOTAL']\n",
    "\n",
    "train['INCOME_CREDIT_RATIO'] = train['AMT_CREDIT'] / train['AMT_INCOME_TOTAL']\n",
    "test['INCOME_CREDIT_RATIO'] = test['AMT_CREDIT'] / test['AMT_INCOME_TOTAL']\n",
    "\n",
    "train['INCOME_PER_CAPITA'] = train['AMT_INCOME_TOTAL'] / train['CNT_FAM_MEMBERS']\n",
    "test['INCOME_PER_CAPITA'] = test['AMT_INCOME_TOTAL'] / test['CNT_FAM_MEMBERS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null with mean for _1, _2, _3\n",
    "mean1 = train['EXT_SOURCE_1'].mean()\n",
    "mean2 = train['EXT_SOURCE_2'].mean()\n",
    "mean3 = train['EXT_SOURCE_3'].mean()\n",
    "train['EXT_SOURCE_1'].fillna(mean1,inplace=True)\n",
    "train['EXT_SOURCE_2'].fillna(mean2,inplace=True)\n",
    "train['EXT_SOURCE_3'].fillna(mean3,inplace=True)\n",
    "test['EXT_SOURCE_1'].fillna(mean1,inplace=True)\n",
    "test['EXT_SOURCE_2'].fillna(mean2,inplace=True)\n",
    "test['EXT_SOURCE_3'].fillna(mean3,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill with mean all _AVG, _MEDI, _MODE\n",
    "for col in train.columns.tolist():\n",
    "    if (col.endswith('_AVG') or col.endswith('_MEDI') or col.endswith('_MODE')) and col not in ['FONDKAPREMONT_MODE','HOUSETYPE_MODE',\n",
    "                    'WALLSMATERIAL_MODE','EMERGENCYSTATE_MODE']: \n",
    "        #print (col)\n",
    "        mean = train[col].mean()\n",
    "        train[col].fillna(mean,inplace=True)\n",
    "        test[col].fillna(mean,inplace=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EMERGENCYSTATE_MODE\n",
    "train['EMERGENCYSTATE_MODE'] = train['EMERGENCYSTATE_MODE'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "test['EMERGENCYSTATE_MODE'] = test['EMERGENCYSTATE_MODE'].apply(lambda x: 1 if x == 'Y' else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Fill none with mean or median for all circle\n",
    "for col in train.columns.tolist():\n",
    "    if col.endswith('_CIRCLE'):\n",
    "        mean = train[col].mean()\n",
    "        train[col].fillna(mean,inplace=True)\n",
    "        test[col].fillna(mean,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative numeric, drop where it is null\n",
    "train['DAYS_LAST_PHONE_CHANGE'].fillna(0,inplace=True)\n",
    "test['DAYS_LAST_PHONE_CHANGE'].fillna(0,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all amt_credit req, maybe bin it\n",
    "train['AMT_REQ_CREDIT_BUREAU_YEAR'].mean()\n",
    "\n",
    "for col in train.columns.tolist():\n",
    "    if 'AMT_REQ_CREDIT_BUREAU_' in col:\n",
    "        #print (col)\n",
    "        mean = train[col].mean()\n",
    "        train[col].fillna(mean,inplace=True)\n",
    "        test[col].fillna(mean,inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_objs_num = len(train)\n",
    "dataset = pd.concat(objs=[train, test], axis=0)\n",
    "dataset = pd.get_dummies(dataset, \n",
    "    columns = ['NAME_TYPE_SUITE','NAME_INCOME_TYPE','NAME_EDUCATION_TYPE','NAME_FAMILY_STATUS','NAME_HOUSING_TYPE',\n",
    "            'OCCUPATION_TYPE','WEEKDAY_APPR_PROCESS_START','ORGANIZATION_TYPE','FONDKAPREMONT_MODE',\n",
    "            'HOUSETYPE_MODE','WALLSMATERIAL_MODE'],prefix_sep='__')\n",
    "train = dataset[:train_objs_num]\n",
    "test = dataset[train_objs_num:]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = train.corr()['TARGET'].sort_values()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train['FLAG_CONT_MOBILE'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train.drop(['SK_ID_CURR','TARGET'], axis=1)\n",
    "y = train['TARGET']\n",
    "X_test = test.drop(['SK_ID_CURR','TARGET'], axis=1)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "\n",
    "import gc\n",
    "import csv\n",
    "\n",
    "cnt = 0\n",
    "p_buf = []\n",
    "n_splits = 2\n",
    "n_repeats = 1\n",
    "kf = RepeatedKFold(\n",
    "    n_splits=n_splits, \n",
    "    n_repeats=n_repeats, \n",
    "    random_state=0)\n",
    "auc_buf = []   \n",
    "\n",
    "params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'max_depth': 12,\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.025,\n",
    "        'feature_fraction': 0.85,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0,\n",
    "        'num_threads': 8,\n",
    "        'lambda_l2': 1.5,\n",
    "        'min_gain_to_split': 0,\n",
    "    }  \n",
    "\n",
    "for train_index, valid_index in kf.split(X):\n",
    "    print('Fold {}/{}'.format(cnt + 1, n_splits))\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb.Dataset(X.loc[train_index], y.loc[train_index], feature_name=X.columns.tolist()),\n",
    "        num_boost_round=10000,\n",
    "        valid_sets=[lgb.Dataset(X.loc[valid_index], y.loc[valid_index])],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=100,\n",
    "    )\n",
    "\n",
    "    if cnt == 0:\n",
    "        importance = model.feature_importance()\n",
    "        model_fnames = model.feature_name()\n",
    "        tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]\n",
    "        tuples = [x for x in tuples if x[1] > 0]\n",
    "        print('Important features:')\n",
    "        print(tuples[:200])\n",
    "\n",
    "    p = model.predict(X.loc[valid_index], num_iteration=model.best_iteration)\n",
    "    #auc = roc_auc_score(y.loc[valid_index], p)\n",
    "\n",
    "    #print('{} AUC: {}'.format(cnt, auc))\n",
    "\n",
    "    p = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "    if len(p_buf) == 0:\n",
    "        p_buf = np.array(p)\n",
    "    else:\n",
    "        p_buf += np.array(p)\n",
    "    #auc_buf.append(auc)\n",
    "\n",
    "    cnt += 1\n",
    "    if cnt > 0: # Comment this to run several folds\n",
    "        pass\n",
    "    \n",
    "    del model\n",
    "    gc.collect\n",
    "\n",
    "#auc_mean = np.mean(auc_buf)\n",
    "#auc_std = np.std(auc_buf)\n",
    "#print('AUC = {:.6f} +/- {:.6f}'.format(auc_mean, auc_std))\n",
    "\n",
    "preds = p_buf/cnt\n",
    "\n",
    "subm = pd.DataFrame()\n",
    "subm['SK_ID_CURR'] = test['SK_ID_CURR']\n",
    "subm['TARGET'] = preds\n",
    "subm.to_csv('home-default-risk_lgbm.csv', index=False,quoting=csv.QUOTE_NONNUMERIC)\n",
    "subm.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"e:/xgboost/python-package\")\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "#model = XGBClassifier()\n",
    "#model.fit(x_train, y_train)\n",
    "#print (model.score(x_train, y_train))\n",
    "\n",
    "logit_model = XGBClassifier() #LogisticRegression()  \n",
    "# Fit\n",
    "#logit_model = logit_model.fit(X_train, y_train,eval_metric='roc_auc')  \n",
    "# How accurate?\n",
    "#print (logit_model.score(X_train, y_train))  \n",
    "#0.7874\n",
    "\n",
    "# How does it perform on the test dataset?\n",
    "# Fit\n",
    "logit_model = logit_model.fit(X, y,eval_metric='auc')  \n",
    "# Predictions on the test dataset\n",
    "predicted = pd.DataFrame(logit_model.predict(X_test))  \n",
    "# Probabilities on the test dataset\n",
    "probs = pd.DataFrame(logit_model.predict_proba(X_test))  \n",
    "#prd[:,i] = np.round(probs,5)[:,1]\n",
    "#print (metrics.accuracy_score(y_test, predicted)  )\n",
    "\n",
    "print (probs.shape)\n",
    "\n",
    "prd_1 = pd.DataFrame(probs)\n",
    "\n",
    "import csv\n",
    "\n",
    "submit = pd.concat([test['SK_ID_CURR'],prd_1],axis=1)\n",
    "\n",
    "print (submit.columns.tolist)\n",
    "\n",
    "submit = submit.drop(submit.columns[1], axis=1)\n",
    "#probs.head()\n",
    "submit.to_csv('home-default-risk_xgb.csv',index=False,quoting=csv.QUOTE_NONNUMERIC)\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_model_and_accuracy(model, params, X, y):\n",
    "    grid = GridSearchCV(model, # the model to grid search\n",
    "                        params, # the parameter set to try \n",
    "                        error_score=0., scoring='roc_auc') # if a parameter set raises an error, continue and set the performance as a big, fat 0\n",
    "    grid.fit(X, y) # fit the model and parameters\n",
    "    # our classical metric for performance\n",
    "    print (\"Best Accuracy: {}\".format(grid.best_score_))\n",
    "    # the best parameters that caused the best accuracy\n",
    "    print (\"Best Parameters: {}\".format(grid.best_params_))\n",
    "    # the average time it took a model to fit to the data (in seconds)\n",
    "    print (\"Average Time to Fit (s): {}\".format(round(grid.cv_results_['mean_fit_time'].mean(), 3)))\n",
    "    # the average time it took a model to predict out of sample data (in seconds)\n",
    "    # this metric gives us insight into how this model will perform in real-time analysis\n",
    "    print (\"Average Time to Score (s): {}\".format(round(grid.cv_results_['mean_score_time'].mean(), 3)))\n",
    "\n",
    "# Logistic Regression\n",
    "lr_params = {'C':[1e-1, 1e0, 1e1, 1e2], 'penalty':['l1', 'l2']}\n",
    "\n",
    "# KNN\n",
    "knn_params = {'n_neighbors': [1, 3, 5, 7]}\n",
    "\n",
    "# Decision Tree\n",
    "tree_params = {'max_depth':[None, 1, 3, 5, 7]}\n",
    "\n",
    "# Random Forest\n",
    "forest_params = {'n_estimators': [10, 50, 100], 'max_depth': [None, 1, 3, 5, 7]}\n",
    "\n",
    "lr = LogisticRegression(penalty='l1',n_jobs=-1)\n",
    "knn = KNeighborsClassifier()\n",
    "d_tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "#get_best_model_and_accuracy(lr, lr_params, X[:3000], y[:3000])\n",
    "#get_best_model_and_accuracy(knn, knn_params, X[:3000], y[:3000])\n",
    "#get_best_model_and_accuracy(d_tree, tree_params, X[:3000], y[:3000])\n",
    "#get_best_model_and_accuracy(forest, forest_params, X[:3000], y[:3000])\n",
    "print ('Fitting...')\n",
    "\n",
    "lr.fit(X,y)\n",
    "probs = lr.predict_proba(X_test)\n",
    "print ('Predicting...')\n",
    "\n",
    "print (lr.score(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_1 = pd.DataFrame(probs)\n",
    "\n",
    "submit = pd.concat([test['SK_ID_CURR'],prd_1],axis=1)\n",
    "\n",
    "print (submit.columns.tolist)\n",
    "\n",
    "submit = submit.drop(submit.columns[1], axis=1)\n",
    "#probs.head()\n",
    "submit.to_csv('home-default-risk.csv',index=False,quoting=csv.QUOTE_NONNUMERIC)\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax1 = sns.distplot(train[\"AMT_CREDIT\"][train.TARGET==1], color='y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.kdeplot(train.loc[train['TARGET'] == 0, 'DAYS_BIRTH'], label = 'Repaid Loan')\n",
    "sns.kdeplot(train.loc[train['TARGET'] == 1, 'DAYS_BIRTH'], label = 'Not Repaid Loan')\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Ages');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
