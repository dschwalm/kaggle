{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def get_best_model_and_accuracy(model, params, X, y):\n",
    "    grid = GridSearchCV(model, # the model to grid search\n",
    "                        params, # the parameter set to try \n",
    "                        error_score=0.) # if a parameter set raises an error, continue and set the performance as a big, fat 0\n",
    "    grid.fit(X, y) # fit the model and parameters\n",
    "    # our classical metric for performance\n",
    "    print (\"Best Accuracy: {}\".format(grid.best_score_))\n",
    "    # the best parameters that caused the best accuracy\n",
    "    print (\"Best Parameters: {}\".format(grid.best_params_))\n",
    "    # the average time it took a model to fit to the data (in seconds)\n",
    "    print (\"Average Time to Fit (s): {}\".format(round(grid.cv_results_['mean_fit_time'].mean(), 3)))\n",
    "    # the average time it took a model to predict out of sample data (in seconds)\n",
    "    # this metric gives us insight into how this model will perform in real-time analysis\n",
    "    print (\"Average Time to Score (s): {}\".format(round(grid.cv_results_['mean_score_time'].mean(), 3)))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "root = '/Users/schwalmdaniel/github/kaggle/titanic'\n",
    "#root = 'd:/dev/python/kaggle/titanic'\n",
    "\n",
    "train=pd.read_csv(root + \"/train.csv\")\n",
    "test=pd.read_csv(root + \"/test.csv\")\n",
    "\n",
    "# have a look at the ds\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Ticket'].unique()\n",
    "\n",
    "train['ticket_prefix'] = train['Ticket'].apply(lambda x: None if x.split(' ')[0].isnumeric() \\\n",
    "                                               else re.sub(r'\\W','', x.split(' ')[0]).lower())\n",
    "test['ticket_prefix'] = test['Ticket'].apply(lambda x: None if x.split(' ')[0].isnumeric() \\\n",
    "                                               else re.sub(r'\\W','', x.split(' ')[0]).lower())\n",
    "test['ticket_prefix'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['name_prefix'] = train['Name'].apply(lambda x: x.lower().strip().split(',')[1].strip().split(' ')[0])\n",
    "test['name_prefix'] = test['Name'].apply(lambda x: x.lower().strip().split(',')[1].strip().split(' ')[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train['Surname'] = train['Name'].apply(lambda x: x.lower().strip().split(',')[0])\n",
    "test['Surname'] = test['Name'].apply(lambda x: x.lower().strip().split(',')[0])\n",
    "\n",
    "train_surnamedf = pd.DataFrame(train.groupby('Surname').size())\n",
    "test_surnamedf = pd.DataFrame(test.groupby('Surname').size())\n",
    "\n",
    "train = train.join(train_surnamedf,on='Surname')\n",
    "train = train.rename(index=str, columns={0: \"same_surname\"})\n",
    "test = test.join(test_surnamedf,on='Surname')\n",
    "test = test.rename(index=str, columns={0: \"same_surname\"})\n",
    "\n",
    "train = train.drop(['Surname'],axis=1)\n",
    "test = test.drop(['Surname'],axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_surnamedf = pd.DataFrame(train.groupby('Ticket').size())\n",
    "test_surnamedf = pd.DataFrame(test.groupby('Ticket').size())\n",
    "\n",
    "train = train.join(train_surnamedf,on='Ticket')\n",
    "train = train.rename(index=str, columns={0: \"same_ticket\"})\n",
    "test = test.join(test_surnamedf,on='Ticket')\n",
    "test = test.rename(index=str, columns={0: \"same_ticket\"})\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null accuracy\n",
    "train['Survived'].value_counts(normalize=True) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# look at the heatmap of the correlation matrix of our dataset\n",
    "sns.heatmap(train.corr())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# numerical correlations\n",
    "train.corr()['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop complicated columns first\n",
    "#train = train.drop(['Name','Cabin','Ticket'],axis=1)\n",
    "#test = test.drop(['Name','Cabin','Ticket'],axis=1)\n",
    "train = train.drop(['Name','Ticket'],axis=1)\n",
    "test = test.drop(['Name','Ticket'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Cabin'].fillna('Unknown', inplace=True)\n",
    "test['Cabin'].fillna('Unknown', inplace=True)\n",
    "\n",
    "train['deck'] = train['Cabin'].apply(lambda x: re.sub(r'[\\d ]','',x[:len(x) if x.find(' ') < 0 else x.find(' ') ]))\n",
    "test['deck'] = test['Cabin'].apply(lambda x: re.sub(r'[\\d ]','',x[:len(x) if x.find(' ') < 0 else x.find(' ') ]))\n",
    "\n",
    "train['multicabin'] = train['Cabin'].apply(lambda x: 1 if ' ' in x else 0 )\n",
    "test['multicabin'] = test['Cabin'].apply(lambda x:  1 if ' ' in x else 0)\n",
    "\n",
    "#train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n",
    "#test['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n",
    "\n",
    "train = train.drop(['Cabin'],axis=1)\n",
    "test = test.drop(['Cabin'],axis=1)\n",
    "\n",
    "#train['deck'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummify features\n",
    "\n",
    "train = pd.get_dummies(train, \n",
    "               columns = ['Pclass', 'Sex', 'Embarked','deck','ticket_prefix','name_prefix'],  # which columns to dummify\n",
    "               prefix_sep='__')  # the separator between the prefix (column name) and cell value\n",
    "\n",
    "train = train.drop(['Sex__male'],axis=1) # drop because of dummy trap\n",
    "test = pd.get_dummies(test, \n",
    "               columns = ['Pclass', 'Sex', 'Embarked','deck','ticket_prefix','name_prefix'],  # which columns to dummify\n",
    "               prefix_sep='__')  # the separator between the prefix (column name) and cell value\n",
    "\n",
    "test = test.drop(['Sex__male'],axis=1) # drop because of dummy trap\n",
    "train = train.drop(['deck__T'],axis=1) # drop because test set does not contain it\n",
    "\n",
    "test = test.drop(list((set([x for x in test.columns.tolist() if 'ticket_prefix' in x or 'name_prefix' in x])\\\n",
    "                     ^ set([x for x in train.columns.tolist() if 'ticket_prefix' in x or 'name_prefix' in x]))),axis=1,errors='ignore')\n",
    "train = train.drop(list((set([x for x in test.columns.tolist() if 'ticket_prefix' in x or 'name_prefix' in x])\\\n",
    "                     ^ set([x for x in train.columns.tolist() if 'ticket_prefix' in x or 'name_prefix' in x]))),axis=1,errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print (set(test.columns.tolist()) ^ set(train.columns.tolist()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sns.heatmap(train.corr())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# imput null ages with mean TODO: check age survived\n",
    "imputer = Imputer(strategy='mean')\n",
    "\n",
    "train = pd.DataFrame(imputer.fit_transform(train), columns=train.columns.tolist())\n",
    "test = pd.DataFrame(imputer.fit_transform(test), columns=test.columns.tolist())\n",
    "\n",
    "#train['Age'] = imputer.fit_transform(train['Age']).ravel()\n",
    "\n",
    "#train['Age'].fillna(train['Age'].mean(), inplace=True)\n",
    "#test['Age'].fillna(test['Age'].mean(), inplace=True)\n",
    "\n",
    "# imput null ages 0\n",
    "#train['Age'].fillna(0,inplace=True)\n",
    "\n",
    "#train['Age'].unique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train['Age'].fillna(train['Age'].mean(), inplace=True)\n",
    "\n",
    "X.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing fare\n",
    "\n",
    "#test['Fare'].fillna(test['Fare'].mean(), inplace=True)\n",
    "\n",
    "X = train.drop(['PassengerId','Survived'], axis=1)\n",
    "# create our feature matrix by removing the response variable\n",
    "print (\"learning from {} rows\".format(X.shape[0]))\n",
    "y = train['Survived']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "lr_params = {'C':[1e-1, 1e0, 1e1, 1e2], 'penalty':['l1', 'l2']}\n",
    "\n",
    "# KNN\n",
    "knn_params = {'n_neighbors': [1, 3, 5, 7]}\n",
    "\n",
    "# Decision Tree\n",
    "tree_params = {'max_depth':[None, 1, 3, 5, 7]}\n",
    "\n",
    "# Random Forest\n",
    "forest_params = {'n_estimators': [10, 50, 100], 'max_depth': [None, 1, 3, 5, 7]}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "d_tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "get_best_model_and_accuracy(lr, lr_params, X, y)\n",
    "get_best_model_and_accuracy(knn, knn_params, X, y)\n",
    "get_best_model_and_accuracy(d_tree, tree_params, X, y)\n",
    "get_best_model_and_accuracy(forest, forest_params, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=7,n_estimators=50)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "\n",
    "pipeline = Pipeline([('imputer', Imputer(strategy='mean')), ('poly_features', poly), ('classify', rf)])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "poly = PolynomialFeatures(degree=1, include_bias=False, interaction_only=False)\n",
    "\n",
    "pipe_params = {'poly_features__degree':[1], 'poly_features__interaction_only':[True], \n",
    "               'classify__n_neighbors':[3, 4, 5, 6]}\n",
    "\n",
    "knn_params = {'classify__n_neighbors':[1, 2, 3, 4, 5, 6, 7]}\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "DecisionTreeClassifier\n",
    "\n",
    "pipeline = Pipeline([('imputer', Imputer(strategy='mean')),  ('standardize', MinMaxScaler()), ('classify', knn)])\n",
    "\n",
    "grid = GridSearchCV(pipeline, pipe_params)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print (grid.best_score_, grid.best_params_)\n",
    "\n",
    "#X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = train['Survived']\n",
    "x_test = test.drop(['PassengerId'],axis=1)\n",
    "\n",
    "pipeline.fit(X,y)\n",
    "preds = pipeline.predict(x_test)\n",
    "\n",
    "print (pipeline.score(X,y))\n",
    "\n",
    "predicted = pd.DataFrame()\n",
    "predicted['PassengerId'] = test['PassengerId']\n",
    "predicted['Survived'] = preds\n",
    "predicted[['PassengerId', 'Survived']] = predicted[['PassengerId', 'Survived']].astype(int)\n",
    "predicted.to_csv(root + '/submission.csv', index=False,quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "predicted.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_test = train['Survived']\n",
    "x_test = test.drop(['PassengerId'],axis=1)\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import KFold, RepeatedKFold\n",
    "\n",
    "import gc\n",
    "import csv\n",
    "\n",
    "cnt = 0\n",
    "p_buf = []\n",
    "n_splits = 2\n",
    "n_repeats = 1\n",
    "kf = RepeatedKFold(\n",
    "    n_splits=n_splits, \n",
    "    n_repeats=n_repeats, \n",
    "    random_state=0)\n",
    "auc_buf = []   \n",
    "\n",
    "params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'max_depth': 12,\n",
    "        'num_leaves': 127,\n",
    "        'learning_rate': 0.025,\n",
    "        'num_iterations': 2000,\n",
    "        'feature_fraction': 0.85,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 0,\n",
    "        'num_threads': 2,\n",
    "        'lambda_l2': 1,\n",
    "        'min_gain_to_split': 0\n",
    "    }  \n",
    "\n",
    "for train_index, valid_index in kf.split(X):\n",
    "    print('Fold {}/{}'.format(cnt + 1, n_splits))\n",
    "    \n",
    "    model = lgb.train(\n",
    "        params,\n",
    "        lgb.Dataset(X.loc[train_index], y.loc[train_index], feature_name=X.columns.tolist()),\n",
    "        num_boost_round=10000,\n",
    "        valid_sets=[lgb.Dataset(X.loc[valid_index], y.loc[valid_index])],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=100,\n",
    "    )\n",
    "\n",
    "    if cnt == 0:\n",
    "        importance = model.feature_importance()\n",
    "        model_fnames = model.feature_name()\n",
    "        tuples = sorted(zip(model_fnames, importance), key=lambda x: x[1])[::-1]\n",
    "        tuples = [x for x in tuples if x[1] > 0]\n",
    "        print('Important features:')\n",
    "        print(tuples[:200])\n",
    "\n",
    "    p = model.predict(X.loc[valid_index], num_iteration=model.best_iteration)\n",
    "    auc = roc_auc_score(y.loc[valid_index], p)\n",
    "\n",
    "    print('{} AUC: {}'.format(cnt, auc))\n",
    "\n",
    "    p = model.predict(x_test, num_iteration=model.best_iteration)\n",
    "    if len(p_buf) == 0:\n",
    "        p_buf = np.array(p)\n",
    "    else:\n",
    "        p_buf += np.array(p)\n",
    "    auc_buf.append(auc)\n",
    "\n",
    "    cnt += 1\n",
    "    if cnt > 0: # Comment this to run several folds\n",
    "        break\n",
    "    \n",
    "    del model\n",
    "    gc.collect\n",
    "\n",
    "auc_mean = np.mean(auc_buf)\n",
    "auc_std = np.std(auc_buf)\n",
    "print('AUC = {:.6f} +/- {:.6f}'.format(auc_mean, auc_std))\n",
    "\n",
    "preds = p_buf/cnt\n",
    "\n",
    "print (type(preds))\n",
    "\n",
    "scale = lambda x: int(round(x))\n",
    "\n",
    "vfunc = np.vectorize(scale)\n",
    "print (vfunc(preds))\n",
    "\n",
    "\n",
    "predicted = pd.DataFrame()\n",
    "predicted['PassengerId'] = test['PassengerId']\n",
    "predicted['Survived'] = vfunc(preds)\n",
    "predicted[['PassengerId', 'Survived']] = predicted[['PassengerId', 'Survived']].astype(int)\n",
    "predicted.to_csv(root + '/submission2.csv', index=False,quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "predicted.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
