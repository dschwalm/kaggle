{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def get_best_model_and_accuracy(model, params, X, y):\n",
    "    grid = GridSearchCV(model, # the model to grid search\n",
    "                        params, # the parameter set to try \n",
    "                        error_score=0.) # if a parameter set raises an error, continue and set the performance as a big, fat 0\n",
    "    grid.fit(X, y) # fit the model and parameters\n",
    "    # our classical metric for performance\n",
    "    print (\"Best Accuracy: {}\".format(grid.best_score_))\n",
    "    # the best parameters that caused the best accuracy\n",
    "    print (\"Best Parameters: {}\".format(grid.best_params_))\n",
    "    # the average time it took a model to fit to the data (in seconds)\n",
    "    print (\"Average Time to Fit (s): {}\".format(round(grid.cv_results_['mean_fit_time'].mean(), 3)))\n",
    "    # the average time it took a model to predict out of sample data (in seconds)\n",
    "    # this metric gives us insight into how this model will perform in real-time analysis\n",
    "    print (\"Average Time to Score (s): {}\".format(round(grid.cv_results_['mean_score_time'].mean(), 3)))\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "root = '/Users/schwalmdaniel/github/kaggle/titanic'\n",
    "#root = 'd:/dev/python/kaggle/titanic'\n",
    "\n",
    "train=pd.read_csv(root + \"/train.csv\")\n",
    "test=pd.read_csv(root + \"/test.csv\")\n",
    "\n",
    "# have a look at the ds\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Ticket'].unique()\n",
    "\n",
    "train['ticket_prefix'] = train['Ticket'].apply(lambda x: None if x.split(' ')[0].isnumeric() \\\n",
    "                                               else re.sub(r'\\W','', x.split(' ')[0]).lower())\n",
    "test['ticket_prefix'] = test['Ticket'].apply(lambda x: None if x.split(' ')[0].isnumeric() \\\n",
    "                                               else re.sub(r'\\W','', x.split(' ')[0]).lower())\n",
    "test['ticket_prefix'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['name_prefix'] = train['Name'].apply(lambda x: x.lower().strip().split(',')[1].strip().split(' ')[0])\n",
    "test['name_prefix'] = test['Name'].apply(lambda x: x.lower().strip().split(',')[1].strip().split(' ')[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check null accuracy\n",
    "train['Survived'].value_counts(normalize=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the heatmap of the correlation matrix of our dataset\n",
    "sns.heatmap(train.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical correlations\n",
    "train.corr()['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop complicated columns first\n",
    "#train = train.drop(['Name','Cabin','Ticket'],axis=1)\n",
    "#test = test.drop(['Name','Cabin','Ticket'],axis=1)\n",
    "train = train.drop(['Name','Ticket'],axis=1)\n",
    "test = test.drop(['Name','Ticket'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Cabin'].fillna('Unknown', inplace=True)\n",
    "test['Cabin'].fillna('Unknown', inplace=True)\n",
    "\n",
    "train['deck'] = train['Cabin'].apply(lambda x: re.sub(r'[\\d ]','',x[:len(x) if x.find(' ') < 0 else x.find(' ') ]))\n",
    "test['deck'] = test['Cabin'].apply(lambda x: re.sub(r'[\\d ]','',x[:len(x) if x.find(' ') < 0 else x.find(' ') ]))\n",
    "\n",
    "train['multicabin'] = train['Cabin'].apply(lambda x: 1 if ' ' in x else 0 )\n",
    "test['multicabin'] = test['Cabin'].apply(lambda x:  1 if ' ' in x else 0)\n",
    "\n",
    "train = train.drop(['Cabin'],axis=1)\n",
    "test = test.drop(['Cabin'],axis=1)\n",
    "\n",
    "train['deck'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummify features\n",
    "\n",
    "train = pd.get_dummies(train, \n",
    "               columns = ['Pclass', 'Sex', 'Embarked','deck','ticket_prefix','name_prefix'],  # which columns to dummify\n",
    "               prefix_sep='__')  # the separator between the prefix (column name) and cell value\n",
    "\n",
    "train = train.drop(['Sex__male'],axis=1) # drop because of dummy trap\n",
    "test = pd.get_dummies(test, \n",
    "               columns = ['Pclass', 'Sex', 'Embarked','deck','ticket_prefix','name_prefix'],  # which columns to dummify\n",
    "               prefix_sep='__')  # the separator between the prefix (column name) and cell value\n",
    "\n",
    "test = test.drop(['Sex__male'],axis=1) # drop because of dummy trap\n",
    "train = train.drop(['deck__T'],axis=1) # drop because test set does not contain it\n",
    "\n",
    "test = test.drop(list((set([x for x in test.columns.tolist() if 'ticket_prefix' in x or 'name_prefix' in x])\\\n",
    "                     ^ set([x for x in train.columns.tolist() if 'ticket_prefix' in x or 'name_prefix' in x]))),axis=1,errors='ignore')\n",
    "train = train.drop(list((set([x for x in test.columns.tolist() if 'ticket_prefix' in x or 'name_prefix' in x])\\\n",
    "                     ^ set([x for x in train.columns.tolist() if 'ticket_prefix' in x or 'name_prefix' in x]))),axis=1,errors='ignore')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (set(test.columns.tolist()) ^ set(train.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(train.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imput null ages with mean TODO: check age survived\n",
    "imputer = Imputer(strategy='mean')\n",
    "\n",
    "train = pd.DataFrame(imputer.fit_transform(train), columns=train.columns.tolist())\n",
    "test = pd.DataFrame(imputer.fit_transform(test), columns=test.columns.tolist())\n",
    "\n",
    "#train['Age'] = imputer.fit_transform(train['Age']).ravel()\n",
    "\n",
    "#train['Age'].fillna(train['Age'].mean(), inplace=True)\n",
    "#test['Age'].fillna(test['Age'].mean(), inplace=True)\n",
    "\n",
    "# imput null ages 0\n",
    "#train['Age'].fillna(0,inplace=True)\n",
    "\n",
    "#train['Age'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing fare\n",
    "\n",
    "#test['Fare'].fillna(test['Fare'].mean(), inplace=True)\n",
    "\n",
    "X = train.drop(['PassengerId','Survived'], axis=1)\n",
    "# create our feature matrix by removing the response variable\n",
    "print (\"learning from {} rows\".format(X.shape[0]))\n",
    "y = train['Survived']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "lr_params = {'C':[1e-1, 1e0, 1e1, 1e2], 'penalty':['l1', 'l2']}\n",
    "\n",
    "# KNN\n",
    "knn_params = {'n_neighbors': [1, 3, 5, 7]}\n",
    "\n",
    "# Decision Tree\n",
    "tree_params = {'max_depth':[None, 1, 3, 5, 7]}\n",
    "\n",
    "# Random Forest\n",
    "forest_params = {'n_estimators': [10, 50, 100], 'max_depth': [None, 1, 3, 5, 7]}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "knn = KNeighborsClassifier()\n",
    "d_tree = DecisionTreeClassifier()\n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "get_best_model_and_accuracy(lr, lr_params, X, y)\n",
    "get_best_model_and_accuracy(knn, knn_params, X, y)\n",
    "get_best_model_and_accuracy(d_tree, tree_params, X, y)\n",
    "get_best_model_and_accuracy(forest, forest_params, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=7,n_estimators=50)\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "\n",
    "pipeline = Pipeline([('imputer', Imputer(strategy='mean')), ('poly_features', poly), ('classify', rf)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=1, include_bias=False, interaction_only=False)\n",
    "\n",
    "pipe_params = {'poly_features__degree':[1], 'poly_features__interaction_only':[True], \n",
    "               'classify__n_neighbors':[3, 4, 5, 6]}\n",
    "\n",
    "knn_params = {'classify__n_neighbors':[1, 2, 3, 4, 5, 6, 7]}\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "pipeline = Pipeline([('imputer', Imputer(strategy='mean')), ('poly_features', poly), \\\n",
    "                     ('standardize', MinMaxScaler()), ('classify', knn)])\n",
    "\n",
    "grid = GridSearchCV(pipeline, pipe_params)\n",
    "grid.fit(X, y)\n",
    "\n",
    "print (grid.best_score_, grid.best_params_)\n",
    "\n",
    "#X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = train['Survived']\n",
    "x_test = test.drop(['PassengerId'],axis=1)\n",
    "\n",
    "pipeline.fit(X,y)\n",
    "preds = pipeline.predict(x_test)\n",
    "\n",
    "print (pipeline.score(X,y))\n",
    "\n",
    "predicted = pd.DataFrame()\n",
    "predicted['PassengerId'] = test['PassengerId']\n",
    "predicted['Survived'] = preds\n",
    "predicted[['PassengerId', 'Survived']] = predicted[['PassengerId', 'Survived']].astype(int)\n",
    "predicted.to_csv(root + '/submission.csv', index=False,quoting=csv.QUOTE_NONNUMERIC)\n",
    "\n",
    "predicted.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
