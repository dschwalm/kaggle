{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import re, os, random\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# reproducible results\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(37)\n",
    "random.seed(17)\n",
    "\n",
    "pd.set_option('display.max_rows',1000)\n",
    "pd.set_option('display.max_columns',150)\n",
    "pd.set_option('max_colwidth', 1000)\n",
    "\n",
    "#root = '/Users/schwalmdaniel/github/kaggle/ml_training/session3'\n",
    "root = 'C:/Users/Administrator/kaggle/ml_training/session3'\n",
    "\n",
    "# data explanation here: https://rstudio-pubs-static.s3.amazonaws.com/155304_cc51f448116744069664b35e7762999f.html\n",
    "\n",
    "train=pd.read_csv(root + \"/spam.csv\", usecols=['v1','v2'])\n",
    "\n",
    "# have a look at the ds\n",
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's see what is the shape of the data (cols, rows)\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the target variable into numeric\n",
    "\n",
    "train['v1'] = train['v1'].apply(lambda x: 1 if x == 'spam' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['v1'].value_counts()\n",
    "\n",
    "# 15% of the rows are spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the most obvious numeric feature of a text is its length\n",
    "\n",
    "train['v2_len'] = train['v2'].apply(lambda x: len(x.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert everything to lowercase\n",
    "\n",
    "train['v2'] = train['v2'].str.lower()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train[train['v1']==1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import string\n",
    "\n",
    "train['v2'] = train['v2'].apply(lambda x : \"\".join([ch for ch in x if ch not in string.punctuation]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['no_of_num'] =  train['v2'].apply(lambda x : sum([1 for ch in x if ch in string.digits]))\n",
    "#train['no_of_punc'] =  train['v2'].apply(lambda x : sum([1 for ch in x if ch in ['?','!']]))\n",
    "#train['no_of_punc'] =  train['v2'].apply(lambda x : sum([int(ch) for ch in x if ch in string.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train['v2'].tolist()\n",
    "corpus[:10]\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corpus = train['v2'].tolist()\n",
    "list([x.split() for x in corpus])[:10]\n",
    "#len(\"\".join([x for x in corpus]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_features = 400\n",
    "tfidf = TfidfVectorizer(max_features=n_features, min_df=2, analyzer='word',strip_accents='unicode')\n",
    "tfidf.fit(corpus)\n",
    "\n",
    "tfidf_train = np.array(tfidf.transform(train['v2']).todense(), dtype=np.float16)\n",
    "\n",
    "for i in range(n_features):\n",
    "    train['v2_tfidf_' + str(i)] = tfidf_train[:, i]\n",
    "    \n",
    "del tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 450\n",
    "mystopwords = (stopwords.words('english')) + list(stop_words.ENGLISH_STOP_WORDS)\n",
    "tfidf = TfidfVectorizer(max_features=n_features,stop_words=mystopwords)\n",
    "tfidf.fit(corpus)\n",
    "\n",
    "tfidf_train = np.array(tfidf.transform(train['v2']).todense(), dtype=np.float16)\n",
    "\n",
    "for i in range(n_features):\n",
    "    train['v2_tfidf_' + str(i)] = tfidf_train[:, i]\n",
    "    \n",
    "del tfidf_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "count_vect = CountVectorizer(max_features=n_features, min_df=2,\n",
    "                            analyzer='word',strip_accents='unicode')\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "weights = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': count_vect.get_feature_names(), 'weight': weights})\n",
    "weights_df = weights_df.sort_values(ascending=False, by=['weight'])\n",
    "weights_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(max_features=n_features,stop_words=mystopwords)\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "weights = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': count_vect.get_feature_names(), 'weight': weights})\n",
    "weights_df = weights_df.sort_values(ascending=False, by=['weight'])\n",
    "weights_df.head(50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mystopwords = (stopwords.words('english')) + list(stop_words.ENGLISH_STOP_WORDS) \n",
    "n_features = 400\n",
    "tfidf = TfidfVectorizer(stop_words=mystopwords, max_features=n_features, min_df=2,\n",
    "                        analyzer='word',ngram_range=(1,3),strip_accents='unicode')\n",
    "tfidf.fit(corpus)\n",
    "\n",
    "tfidf_train = np.array(tfidf.transform(train['v2']).todense(), dtype=np.float16)\n",
    "\n",
    "for i in range(n_features):\n",
    "    train['v2_tfidf_' + str(i)] = tfidf_train[:, i]\n",
    "    \n",
    "del tfidf_train\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=mystopwords,max_features=n_features, min_df=2,\n",
    "                            analyzer='word',ngram_range=(1,3),strip_accents='unicode')\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "weights = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': count_vect.get_feature_names(), 'weight': weights})\n",
    "weights_df = weights_df.sort_values(ascending=False, by=['weight'])\n",
    "weights_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the textual feature weights are all between 0 and 1, rescale all other numeric features to this range\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train[['v2_len','no_of_num']])\n",
    "train[['v2_len','no_of_num']] = scaler.transform(train[['v2_len','no_of_num']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn fp values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = train[['v1','v2']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['v1','v2'], axis=1)\n",
    "y = train['v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, df.index, test_size=0.10, random_state=42, shuffle=True)\n",
    "print ('Training shape: %s, test shape: %s' % (X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=42)\n",
    "forest.fit(X_train, y_train)\n",
    "predictions = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, predictions)\n",
    "sns.heatmap(conf_mat, annot=True, fmt=\".0f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we predicted spam but not spam\n",
    "mis = df.loc[indices_test[(y_test == 0) & (predictions == 1)]][['v2']]\n",
    "mis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we predicted not spam but spam\n",
    "mis = df.loc[indices_test[(y_test == 1) & (predictions == 0)]][['v2']]\n",
    "mis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information extraction examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ChunkParserI\n",
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.corpus import conll2000\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#nltk.download('conll2000')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "        \n",
    "    return backoff\n",
    "\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "    \n",
    "    return [[(t,c) for (w,t,c) in sent] for sent in tagged_sents]\n",
    "\n",
    "class TagChunker(ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_chunks, tagger_classes=[UnigramTagger, BigramTagger]):\n",
    "        train_sents = conll_tag_chunks(train_chunks)\n",
    "        self.tagger = backoff_tagger(train_sents, tagger_classes)\n",
    "        \n",
    "    def parse(self, tagged_sent):\n",
    "        if not tagged_sent:\n",
    "            return None\n",
    "        \n",
    "        (words, tags) = zip(*tagged_sent)\n",
    "        chunks = self.tagger.tag(tags)\n",
    "        \n",
    "        wtc = zip(words, chunks)\n",
    "        \n",
    "        return conlltags2tree([(w,t,c) for (w, (t,c)) in wtc])\n",
    "\n",
    "# first we have to pre-train our model from NLTK factory training set    \n",
    "    \n",
    "conll_train = conll2000.chunked_sents('train.txt')\n",
    "chunker = TagChunker(conll_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we can POS tag and visualize our sentence\n",
    "# you can find the description of all POS tags here: \n",
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "inputString = 'That is one small step for man, one giant leap for mankind'\n",
    "\n",
    "pos_tagged = nltk.pos_tag(word_tokenize(inputString))\n",
    "chunker.parse(pos_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# python -m spacy download en\n",
    "\n",
    "sentence = '''European Union fined Google for a record of $5.1bn on Monday'''\n",
    "\n",
    "displacy.render(nlp(str(sentence)), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp(sentence), jupyter=True, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "testimonial = TextBlob(\"The teacher is beautiful!\")\n",
    "testimonial.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testimonial = TextBlob(\"The teacher is very ugly!\")\n",
    "testimonial.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testimonial = TextBlob(\"That is crap. Some think the population of Hungary is 10 million\")\n",
    "testimonial.sentiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
