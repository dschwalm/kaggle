{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import re, os, random\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# reproducible results\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(37)\n",
    "random.seed(17)\n",
    "\n",
    "pd.set_option('display.max_rows',1000)\n",
    "pd.set_option('display.max_columns',150)\n",
    "\n",
    "root = '/Users/schwalmdaniel/github/kaggle/ml_training/session3'\n",
    "#root = 'e:/kaggle/ml_training/session3'\n",
    "\n",
    "# data explanation here: https://rstudio-pubs-static.s3.amazonaws.com/155304_cc51f448116744069664b35e7762999f.html\n",
    "\n",
    "train=pd.read_csv(root + \"/spam.csv\", usecols=['v1','v2'])\n",
    "\n",
    "# have a look at the ds\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's see what is the shape of the data (cols, rows)\n",
    "\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the target variable into numeric\n",
    "\n",
    "train['v1'] = train['v1'].apply(lambda x: 1 if x == 'spam' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['v1'].value_counts()\n",
    "\n",
    "# 15% of the rows are spam"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# the most obvious numeric feature of a text is its length\n",
    "\n",
    "train['v2_len'] = train['v2'].apply(lambda x: len(x.strip()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# convert everything to lowercase\n",
    "\n",
    "train['v2'] = train['v2'].str.lower()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train[train['v1']==1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import string\n",
    "\n",
    "train['v2'] = train['v2'].apply(lambda x : \"\".join([ch for ch in x if ch not in string.punctuation]))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train['no_of_num'] =  train['v2'].apply(lambda x : sum([1 for ch in x if ch in string.digits]))\n",
    "#train['no_of_punc'] =  train['v2'].apply(lambda x : sum([1 for ch in x if ch in ['?','!']]))\n",
    "#train['no_of_punc'] =  train['v2'].apply(lambda x : sum([int(ch) for ch in x if ch in string.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train['v2'].tolist()\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "corpus = train['v2'].tolist()\n",
    "list([x.split() for x in corpus])[:10]\n",
    "#len(\"\".join([x for x in corpus]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 400\n",
    "tfidf = TfidfVectorizer(max_features=n_features, min_df=2, analyzer='word',strip_accents='unicode')\n",
    "tfidf.fit(corpus)\n",
    "\n",
    "tfidf_train = np.array(tfidf.transform(train['v2']).todense(), dtype=np.float16)\n",
    "\n",
    "for i in range(n_features):\n",
    "    train['v2_tfidf_' + str(i)] = tfidf_train[:, i]\n",
    "    \n",
    "del tfidf_train"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_features = 450\n",
    "mystopwords = (stopwords.words('english')) + list(stop_words.ENGLISH_STOP_WORDS)\n",
    "tfidf = TfidfVectorizer(max_features=n_features,stop_words=mystopwords)\n",
    "tfidf.fit(corpus)\n",
    "\n",
    "tfidf_train = np.array(tfidf.transform(train['v2']).todense(), dtype=np.float16)\n",
    "\n",
    "for i in range(n_features):\n",
    "    train['v2_tfidf_' + str(i)] = tfidf_train[:, i]\n",
    "    \n",
    "del tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(max_features=n_features, min_df=2,\n",
    "                            analyzer='word',strip_accents='unicode')\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "weights = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': count_vect.get_feature_names(), 'weight': weights})\n",
    "weights_df = weights_df.sort_values(ascending=False, by=['weight'])\n",
    "weights_df.head(50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "count_vect = CountVectorizer(max_features=n_features,stop_words=mystopwords)\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "weights = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': count_vect.get_feature_names(), 'weight': weights})\n",
    "weights_df = weights_df.sort_values(ascending=False, by=['weight'])\n",
    "weights_df.head(100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mystopwords = (stopwords.words('english')) + list(stop_words.ENGLISH_STOP_WORDS) \n",
    "n_features = 400\n",
    "tfidf = TfidfVectorizer(stop_words=mystopwords, max_features=n_features, min_df=2,\n",
    "                        analyzer='word',ngram_range=(1,3),strip_accents='unicode')\n",
    "tfidf.fit(corpus)\n",
    "\n",
    "tfidf_train = np.array(tfidf.transform(train['v2']).todense(), dtype=np.float16)\n",
    "\n",
    "for i in range(n_features):\n",
    "    train['v2_tfidf_' + str(i)] = tfidf_train[:, i]\n",
    "    \n",
    "del tfidf_train\n",
    "\n",
    "count_vect = CountVectorizer(stop_words=mystopwords,max_features=n_features, min_df=2,\n",
    "                            analyzer='word',ngram_range=(1,3),strip_accents='unicode')\n",
    "X_train_counts = count_vect.fit_transform(corpus)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "weights = np.asarray(X_train_tfidf.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': count_vect.get_feature_names(), 'weight': weights})\n",
    "weights_df = weights_df.sort_values(ascending=False, by=['weight'])\n",
    "weights_df.head(100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# as the textual feature weights are all between 0 and 1, rescale all other numeric features to this range\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train[['v2_len','no_of_num']])\n",
    "train[['v2_len','no_of_num']] = scaler.transform(train[['v2_len','no_of_num']] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(['v1','v2'], axis=1)\n",
    "y = train['v1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42, shuffle=True)\n",
    "print ('Training shape: %s, test shape: %s' % (X_train.shape, X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=42)\n",
    "forest.fit(X_train, y_train)\n",
    "predictions = forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information extraction examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import ChunkParserI\n",
    "from nltk.chunk.util import tree2conlltags, conlltags2tree\n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.corpus import conll2000\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "#nltk.download('conll2000')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "        \n",
    "    return backoff\n",
    "\n",
    "def conll_tag_chunks(chunk_sents):\n",
    "    tagged_sents = [tree2conlltags(tree) for tree in chunk_sents]\n",
    "    \n",
    "    return [[(t,c) for (w,t,c) in sent] for sent in tagged_sents]\n",
    "\n",
    "class TagChunker(ChunkParserI):\n",
    "    \n",
    "    def __init__(self, train_chunks, tagger_classes=[UnigramTagger, BigramTagger]):\n",
    "        train_sents = conll_tag_chunks(train_chunks)\n",
    "        self.tagger = backoff_tagger(train_sents, tagger_classes)\n",
    "        \n",
    "    def parse(self, tagged_sent):\n",
    "        if not tagged_sent:\n",
    "            return None\n",
    "        \n",
    "        (words, tags) = zip(*tagged_sent)\n",
    "        chunks = self.tagger.tag(tags)\n",
    "        \n",
    "        wtc = zip(words, chunks)\n",
    "        \n",
    "        return conlltags2tree([(w,t,c) for (w, (t,c)) in wtc])\n",
    "\n",
    "# first we have to pre-train our model from NLTK factory training set    \n",
    "    \n",
    "conll_train = conll2000.chunked_sents('train.txt')\n",
    "chunker = TagChunker(conll_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we can POS tag and visualize our sentence\n",
    "# you can find the description of all POS tags here: \n",
    "# https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "inputString = 'That is one small step for man, one giant leap for mankind'\n",
    "\n",
    "pos_tagged = nltk.pos_tag(word_tokenize(inputString))\n",
    "chunker.parse(pos_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# python -m spacy download en\n",
    "\n",
    "sentence = '''European authorities fined Google a record $5.1 billion on Wednesday for abusing \n",
    "its power in the mobile phone market and ordered the company to alter its practices'''\n",
    "\n",
    "displacy.render(nlp(str(sentence)), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(nlp('I want an early upgrade'), jupyter=True, style='dep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "testimonial = TextBlob(\"The teacher is beautiful!\")\n",
    "testimonial.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testimonial = TextBlob(\"The teacher is ugly!\")\n",
    "testimonial.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testimonial = TextBlob(\"The population of Hungary is 10 million\")\n",
    "testimonial.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)\n",
    "\n",
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [1, 5, 10],\n",
    "    'max_features': ['auto'],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'min_samples_split': [1.0],\n",
    "    'n_estimators': [100,500, 900]\n",
    "}\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier()\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid = grid_search.best_estimator_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
